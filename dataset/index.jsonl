{"image": "nougat/05.png", "markdown": "remove these elements in a pre-processing step using pdffigures2[9]. The recognized captions are are then compared to the captions in the XML file and matched based on their Levenshtein distance [21]. Once the source document has been split into individual pages, the removed figures and tables are reinserted at the end of each page.\n\nFor a better matching we also replaced unicode characters in the PDF text with corresponding LaTeX commands using the pylatexenc-library7.\n\nFootnote 7: https://github.com/phfaist/pylatexenc\n\n**Bag of Words matching**   First we extract the text lines from the PDF using MuPDF8 and preprocess them to remove page numbers and potential headers/footers. We then use a _Bag of Words_ model [15] with TF-IDF vectorizer and a linear Support Vector Machine classifier. The model is fitted to the PDF lines with the page number as label. Next we split the LaTeX source into paragraphs and predict the page number for each of them.\n\nFootnote 8: https://mupdf.com/\n\nIdeally, the predictions will form a stair case function but in practice the signal will be noisy. To find the best boundary points we employ a similar logic as decision trees and minimize a measure based on the _Gini_ impurity\n\n\\[G_{[a,\\:\\!b]}(i)=(b-a)\\cdot\\left(1-p_{[a,\\:\\!b]}^{2}(i)-p_{[a,\\:\\!b]}^{2}(i+1) \\right),\\]\n\nwhere \\(p_{[a,\\:\\!b]}(i)\\) is the probability of choosing an element with the predicted page number \\(i\\) in the interval \\([a,\\,b]\\) that describes which paragraphs (elements) were considered for the split.\n\nThe best splitting position \\(t\\) in the interval \\([a,\\,b]\\) is then\n\n\\[{\\hat{t}}_{i}=\\operatorname*{\\arg\\,\\min}_{t}\\left(G_{[a,\\:\\!t]}(i)+G_{[t,\\:\\!b ]}(i)\\right).\\]\n\nThe search process starts with all paragraphs and for each subsequent page break, the lower bound of the search interval is set to the previous split position. See Fig. 4 for a visualization of an example page.\n\n\n\n**Fuzzy matching**   After this first coarse document splitting we try to find the exact position within the paragraph. This is done by comparing the source text within the neighborhood of the predicted splitting position to the last sentences of the previous page of the embedded PDF text, and the first sentences of the next page using the fuzzysearch library9. If the two dividing points are at the same location in the source text, the page break is considered \"accurate\" and receives a score of 1. On the other hand, if the splitting positions differ, the one with the smallest normalized Levenshtein distance is selected and given a score of 1 minus the distance. To be included in the dataset, a PDF page must have an average score of at least 0.9 for both page breaks. This results in an acceptance rate of about \\(47\\%\\) of all pages.\n\nFootnote 9: https://github.com/taleinat/fuzzysearch\n\n### Ground truth artifacts\n\nBecause the dataset was pre-processed by LaTeXML, the markup version of the source code can contain artifacts and commands from unsupported packages. The HTML file may contain subsection titles with numbering even though they are not numbered in the PDF. There may also be instances where figures or tables are missing from the ground truth due to processing errors.\n\nFigure 4: Example for splitting the paragraphs in the source code into different pages. The points in blue denote the page index predicted by the SVM.", "meta": [{"caption": "Figure 4: Example for splitting the paragraphs in the source code into different pages. The points in blue denote the page index predicted by the SVM.", "captionBoundary": {"x1": 85, "x2": 709, "y1": 302, "y2": 325}, "figType": "Figure", "imageText": ["Predictions", "Staircase", "\ufb01t", "d", "ex", "e", "in", "P", "ag", "8", "6", "4", "2", "0", "Paragraph", "index", "0", "5", "10", "15", "20", "25", "30", "35", "40"], "name": "4", "regionBoundary": {"x1": 259, "x2": 541, "y1": 108, "y2": 276}, "source": "fig"}]}
{"image": "nougat/04.png", "markdown": "## 4 Datasets\n\nTo the best of our knowledge there is no paired dataset of PDF pages and corresponding source code out there, so we created our own from the open access articles on arXiv.3 For layout diversity we also include a subset of the _PubMed Central_4 (PMC) open access non-commercial dataset. During the pretraining, a portion of the _Industry Documents Library_5 (IDL) is included. See Table A.1 for the dataset composition.\n\nFootnote 3: https://arxiv.org/\n\nFootnote 4: https://www.ncbi.nlm.nih.gov/pmc/\n\nFootnote 5: https://www.industrydocuments.ucsf.edu/\n\n**arXiv**   We collected the source code and compiled PDFs from 1,748,201 articles released on arXiv. To ensure consistent formatting, we first process the source files using _LaTeXML_6 and convert them into HTML5 files. This step was important as it standardized and removed ambiguity from the LaTeX source code, especially in mathematical expressions. The conversion process included replacing user-defined macros, standardizing whitespace, adding optional brackets, normalizing tables, and replacing references and citations with their correct numbers.\n\nFootnote 6: http://dlmf.nist.gov/LaTeXML/\n\nWe then parse the HTML files and convert them into a lightweight markup language that supports various elements such as headings, bold and italic text, algorithms, LaTeX inline and display math and LaTeX tables. This way, we ensure that the source code is properly formatted and ready for further processing.\n\nThe process is visualized in Fig. 3.\n\n\n\n**PMC**   We also processed articles from PMC, where XML files with semantic information are available in addition to the PDF file. We parse these files into the same markup language format as the arXiv articles. We chose to use far fewer articles from PMC because the XML files are not always as rich in semantic information. Often times equations and tables are stored as images and these cases are not trivial to detect, which leads to our decision to limit the use of PMC articles to the pre-training phase.\n\nThe XML files are parsed into the same markup language as described above.\n\n**IDL**   The IDL is a collection of documents produced by industries that have an impact on public health and is maintained by the University of California, San Francisco Library. Biten et al. [6] provide high quality OCR text for PDFs from the IDL dataset. This does not include text formatting and is only used for pre-training to teach the model basic OCR of scanned documents.\n\n### Splitting the pages\n\nWe split the markdown files according to the page breaks in the PDF file and rasterize each page as an image to create the final paired dataset. During the compilation, the LaTeX compiler determines the page breaks of the PDF file automatically. Since we are not recompiling the LaTeX sources for each paper, we must heuristically split the source file into parts, which correspond to different pages. To achieve that we are using the embedded text on the PDF page and match it to source text.\n\nHowever, figures and tables in the PDF may not correspond to their position in the source code. To address this issue, we\n\nFigure 3: Data processing. The source file is converted into HTML which is then converted to Markdown. a) The LaTeX source provided by the authors. b) The HTML file computed form the LaTeX source using LaTeXML. c) The Markdown file parsed from the HTML file. d) The PDF file provided by the authors", "meta": [{"caption": "Figure 3: Data processing. The source file is converted into HTML which is then converted to Markdown. a) The LaTeX source provided by the authors. b) The HTML file computed form the LaTeX source using LaTeXML. c) The Markdown file parsed from the HTML file. d) The PDF file provided by the authors", "captionBoundary": {"x1": 85, "x2": 709, "y1": 274, "y2": 312}, "figType": "Figure", "imageText": ["<h2><span>1", "</span>Title</h2>", "<p>", "We", "study", "the", "formula", "</p>", "<math><mrow>", "<mi>E</mi><mo>=", "<mo>...</math>", "<p>", "as", "in", "</p>", "<cite>[1]</cite>", "b)", "html", "as", "in", "[1]", "\\[E=mc^{2}\\]", "(1)", "We", "study", "the", "formula", "#", "1", "Title", "d)", "pdfc)", "markdown", "We", "study", "the", "formula", "\\begin{equation}", "E=mc^{2}", "\\label{einstein}", "\\end{equation}", "as", "in", "\\cite{ein}", "\\section{Title}", "a)", "tex"], "name": "3", "regionBoundary": {"x1": 101, "x2": 692, "y1": 100, "y2": 257}, "source": "fig"}]}
{"image": "nougat/01.png", "markdown": "# Nougat: Neural Optical Understanding for Academic Documents\n\n Lukas Blecher &Guillem Cucurull &Thomas Scialom &Robert Stojnic Meta AI\n\nCorrespondence to: lblecher@meta.com\n\nJanuary 2023\n\n###### Abstract\n\nScientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (**N**eural **O**ptical **U**nderstandin**g** for **A**cademic Documen**t**s), a Visual Transformer model that performs an _Optical Character Recognition_ (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition.\n\n## 1 Introduction\n\nThe majority of scientific knowledge is stored in books or published in scientific journals, most commonly in the Portable Document Format (PDF). Next to HTML, PDFs are the second most prominent data format on the internet, making up 2.4% of common crawl [35]. However, the information stored in these files is very difficult to extract into any other formats. This is especially true for highly specialized documents, such as scientific research papers, where the semantic information of mathematical expressions is lost.\n\nExisting Optical Character Recognition (OCR) engines, such as Tesseract OCR [39], excel at detecting and classifying individual characters and words in an image, but fail to understand the relationship between them due to their line-by-line approach. This means that they treat superscripts and subscripts in the same way as the surrounding text, which is a significant drawback for mathematical expressions. In mathematical notations like fractions, exponents, and matrices, relative positions of characters are crucial.\n\nConverting academic research papers into machine-readable text also enables accessibility and searchability of science as a whole. The information of millions of academic papers can not be fully accessed because they are locked behind an unreadable format. Existing corpora, such as the S2ORC dataset [25], capture the text of 12M1 papers using GROBID [26], but are missing meaningful representations of the mathematical equations.\n\nFootnote 1: The paper reports 8.1M papers but the authors recently updated the numbers on the GitHub page https://github.com/allenai/s2orc\n\nTo this end, we introduce Nougat, a transformer based model that can convert images of document pages to formatted markup text.\n\nThe primary contributions in this paper are\n\n* Release of a pre-trained model capable of converting a PDF to a lightweight markup language. We release the code and the model on GitHub2 Footnote 2: https://github.com/facebookresearch/nougat\n* We introduce a pipeline to create dataset for pairing PDFs to source code\n* Our method is only dependent on the image of a page, allowing access to scanned papers and books", "meta": []}
