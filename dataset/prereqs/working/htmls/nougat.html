<!DOCTYPE html><html><head>
<title>Nougat: Neural Optical Understanding for Academic Documents</title>
<!--Generated by LaTeXML (version 0.8.4) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on January 2023.-->

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="index.css" type="text/css">

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><style type="text/css">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Nougat: Neural Optical Understanding for Academic Documents</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Lukas Blecher
&amp;Guillem Cucurull
&amp;Thomas Scialom
&amp;Robert Stojnic
<span class="ltx_ERROR undefined">\AND</span>Meta AI

</span><span class="ltx_author_notes"><span>Correspondence to: <a href="mailto:lblecher@meta.com" title="" class="ltx_ref ltx_href">lblecher@meta.com</a></span></span></span>
</div>
<div class="ltx_date ltx_role_creation">January 2023</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Scientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions.
We propose Nougat (<span class="ltx_text ltx_font_bold">N</span>eural <span class="ltx_text ltx_font_bold">O</span>ptical <span class="ltx_text ltx_font_bold">U</span>nderstandin<span class="ltx_text ltx_font_bold">g</span> for <span class="ltx_text ltx_font_bold">A</span>cademic Documen<span class="ltx_text ltx_font_bold">t</span>s), a Visual Transformer model that performs an <em class="ltx_emph ltx_font_italic">Optical Character Recognition</em> (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">The majority of scientific knowledge is stored in books or published in scientific journals, most commonly in the Portable Document Format (PDF).
Next to HTML, PDFs are the second most prominent data format on the internet, making up &nbsp;2.4% of common crawl <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="Statistics of the Common Crawl Corpus 2012" class="ltx_ref">35</a>]</cite>. However, the information stored in these files is very difficult to extract into any other formats. This is especially true for highly specialized documents, such as scientific research papers, where the semantic information of mathematical expressions is lost.
<br class="ltx_break">Existing Optical Character Recognition (OCR) engines, such as Tesseract OCR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="An Overview of the Tesseract OCR Engine" class="ltx_ref">39</a>]</cite>, excel at detecting and classifying individual characters and words in an image,
but fail to understand the relationship between them due to their line-by-line approach. This means that they treat superscripts and subscripts in the same way as the surrounding text, which is a significant drawback for mathematical expressions. In mathematical notations like fractions, exponents, and matrices, relative positions of characters are crucial.
<br class="ltx_break">Converting academic research papers into machine-readable text also enables accessibility and searchability of science as a whole. The information of millions of academic papers can not be fully accessed because they are locked behind an unreadable format. Existing corpora, such as the S2ORC dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="S2ORC: The Semantic Scholar Open Research Corpus" class="ltx_ref">25</a>]</cite>, capture the text of 12M<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The paper reports 8.1M papers but the authors recently updated the numbers on the GitHub page <a href="https://github.com/allenai/s2orc" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/allenai/s2orc</a></span></span></span> papers using GROBID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="GROBID" class="ltx_ref">26</a>]</cite>, but are missing meaningful representations of the mathematical equations.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">To this end, we introduce Nougat, a transformer based model that can convert images of document pages to formatted markup text.

<br class="ltx_break">The primary contributions in this paper are</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item">
<div id="S1.I1.i1.p1" class="ltx_para">
<p class="ltx_p">Release of a pre-trained model capable of converting a PDF to a lightweight markup language. We release the code and the model on GitHub<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a href="https://github.com/facebookresearch/nougat" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/nougat</a></span></span></span></p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item">
<div id="S1.I1.i2.p1" class="ltx_para">
<p class="ltx_p">We introduce a pipeline to create dataset for pairing PDFs to source code</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item">
<div id="S1.I1.i3.p1" class="ltx_para">
<p class="ltx_p">Our method is only dependent on the image of a page, allowing access to scanned papers and books</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">Optical Character Recognition (OCR) is an extensively researched field in computer vision for a variety applications, such as document digitalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="Full-Page Text Recognition: Learning Where to Start and When to Stop" class="ltx_ref">33</a>, <a href="#bib.bib33" title="An Overview of the Tesseract OCR Engine" class="ltx_ref">39</a>]</cite>, handwriting recognition and scene text recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="Scene Text Recognition with Permuted Autoregressive Sequence Models" class="ltx_ref">5</a>, <a href="#bib.bib5" title="TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models" class="ltx_ref">23</a>, <a href="#bib.bib27" title="Rethinking Text Line Recognition Models" class="ltx_ref">12</a>]</cite>.

<br class="ltx_break">More concretely, recognizing mathematical expressions is a heavily researched subtopic. Grammar based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="A new approach for recognizing handwritten mathematics using relational grammars and fuzzy sets" class="ltx_ref">29</a>, <a href="#bib.bib19" title="A global learning approach for an online handwritten mathematical expression recognition system" class="ltx_ref">3</a>, <a href="#bib.bib23" title="Recognition of on-line handwritten mathematical expressions using 2D stochastic context-free grammars and hidden Markov models" class="ltx_ref">1</a>]</cite> for handwritten mathematical expressions were improved upon by different encoder-decoder models. The fully convolutional model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="ConvMath: A Convolutional Sequence Network for Mathematical Expression Recognition" class="ltx_ref">46</a>]</cite> was succeeded by various RNN decoder models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="Image-to-Markup Generation with Coarse-to-Fine Attention" class="ltx_ref">11</a>, <a href="#bib.bib22" title="Training an End-to-End System for Handwritten Mathematical Expression Recognition by Generated Patterns" class="ltx_ref">20</a>, <a href="#bib.bib16" title="Teaching Machines to Code: Neural Markup Generation with Visual Attention" class="ltx_ref">38</a>, <a href="#bib.bib20" title="Multi-Scale Attention with Dense Encoder for Handwritten Mathematical Expression Recognition" class="ltx_ref">47</a>, <a href="#bib.bib4" title="Translating Math Formula Images to LaTeX Sequences Using Deep Neural Networks with Sequence-level Training" class="ltx_ref">43</a>]</cite>, both for handwritten and printed formulas. Recently, the decoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="Handwritten Mathematical Expression Recognition with Bidirectionally Trained Transformer" class="ltx_ref">48</a>, <a href="#bib.bib44" title="ICDAR 2019 CROHME + TFD: Competition on Recognition of Handwritten Mathematical Expressions and Typeset Formula Detection" class="ltx_ref">30</a>]</cite> as well as the encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="Pix2tex - LaTeX OCR" class="ltx_ref">7</a>]</cite> were replaced with the Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="Attention Is All You Need" class="ltx_ref">42</a>]</cite> architecture.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">Visual Document Understanding (VDU) is another related topic of deep learning research and focuses on extracting relevant information of a variety of document types. Previous works depend on pre-trained models that learn to extract information by jointly modeling text and layout information using the Transformer architecture. The LayoutLM model family <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="LayoutLM: Pre-training of Text and Layout for Document Image Understanding" class="ltx_ref">45</a>, <a href="#bib.bib10" title="LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding" class="ltx_ref">44</a>, <a href="#bib.bib11" title="LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking" class="ltx_ref">17</a>]</cite> uses masked layout prediction task to capture the spatial relationships between different document elements.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">Open source solutions with a related goal as ours include GROBID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="GROBID" class="ltx_ref">26</a>]</cite>, which parses digital-born scientific documents to XML with a focus on the bibliographic data and <code class="ltx_verbatim ltx_font_typewriter">pdf2htmlEX</code> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="Online publishing via pdf2htmlEX" class="ltx_ref">28</a>]</cite>, that converts digital-born PDFs to HTML while preserving the layout and appearance of the document. However, both solutions can not recover the semantic information of mathematical expressions.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Model</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">Previous VDU methods either rely on OCR text from a third party tool <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="LayoutLM: Pre-training of Text and Layout for Document Image Understanding" class="ltx_ref">45</a>, <a href="#bib.bib10" title="LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding" class="ltx_ref">44</a>, <a href="#bib.bib6" title="DocFormer: End-to-End Transformer for Document Understanding" class="ltx_ref">2</a>]</cite> or focus on document types such as receipts, invoices or form-like documents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="Representation Learning for Information Extraction from Form-like Documents" class="ltx_ref">31</a>]</cite>.
Recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="OCR-free Document Understanding Transformer" class="ltx_ref">18</a>, <a href="#bib.bib30" title="End-to-end Document Recognition and Understanding with Dessurt" class="ltx_ref">10</a>]</cite> show that an external OCR engine is not necessarily needed to achieve competitive results in VDU.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">The architecture is a encoder-decoder transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="Attention Is All You Need" class="ltx_ref">42</a>]</cite> architecture, that allows for an end-to-end training procedure. We build on the Donut <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="OCR-free Document Understanding Transformer" class="ltx_ref">18</a>]</cite> architecture. The model does not require any OCR related inputs or modules. The text is recognized implicitly by the network. See Fig. <a href="#S3.F1" title="Figure 1 ‣ 3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for an overview of the approach.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Encoder</span>   The visual encoder receives a document image <span id="S3.p3.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{x}\in\mathbb{R}^{3\times H_{0}\times W_{0}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.057em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">H</span></span></span><span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span><span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, crops the margins and resizes the image to fit in a fixed rectangle of size <span id="S3.p3.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(H,\,W)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 0.167em; height: 0px;"></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>. If the image is smaller than the rectangle, additional padding is added to ensure each image has the same dimensionality. We use a Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows" class="ltx_ref">24</a>]</cite>, a hierarchical vision transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" class="ltx_ref">13</a>]</cite> that splits the image into non-overlapping windows of fixed size and applies a series of self-attention layers to aggregate information across these windows. The model output a sequence of the embedded patches <span id="S3.p3.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{z}\in\mathbb{R}^{d\times N}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">z</span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span></span></span></span></span></span></span></span></span></span> where <span id="S3.p3.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="d"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span></span></span></span></span> is the latent dimension and <span id="S3.p3.m5" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="N"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span></span></span></span></span></span> is the number of patches.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Decoder</span>  
The encoded image <span id="S3.p4.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{z}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">z</span></span></span></span></span></span></span></span></span> is decoded into a sequence of tokens using a transformer decoder architecture with cross-attention. The tokens are generated in an auto-regressive manner, using self-attention and cross-attention to attend to different parts of the input sequence and encoder output respectively. Finally, the output is projected to the size of the vocabulary <span id="S3.p4.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="v"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">v</span></span></span></span></span></span></span>, yielding the logits <span id="S3.p4.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\boldsymbol{\ell}\in\mathbb{R}^{v}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">ℓ</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">v</span></span></span></span></span></span></span></span></span></span></span>.
<br class="ltx_break">Following Kim et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="OCR-free Document Understanding Transformer" class="ltx_ref">18</a>]</cite>, we use the implementation of the mBART <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension" class="ltx_ref">22</a>]</cite> decoder. We use the same tokenizer as Taylor et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="Galactica: A Large Language Model for Science" class="ltx_ref">41</a>]</cite> because their model is also specialized in the scientific text domain.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering" width="642" height="148" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" style="font-size:90%;">
Our simple end-to-end architecture followin Donut <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="OCR-free Document Understanding Transformer" class="ltx_ref">18</a>]</cite>. The Swin Transformer encoder takes a document image and converts it into latent embeddings, which are subsequently converted to a sequence of tokens in a auto-regressive manner</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Setup</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">We render the document images at a resolution of 96 DPI. Due to the restrictive possible input dimensions of the Swin Transformer we choose the input size <span id="S3.SS1.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(H,\,W)=(896,\,672)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 0.167em; height: 0px;"></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">896</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 0.167em; height: 0px;"></span><span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">672</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>. The aspect ratio is in between the US letter and Din A4 format <span id="S3.SS1.p1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\frac{22}{17}<\frac{4}{3}<\sqrt{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.849em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 1.2em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">22</span></span></span><span class="mjx-denominator" style="font-size: 70.7%; width: 1.2em; bottom: -0.697em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">17</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.849em;" class="mjx-line"></span></span><span style="height: 1.464em; vertical-align: -0.493em;" class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span><span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.7em; top: -1.383em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span><span class="mjx-denominator" style="font-size: 70.7%; width: 0.7em; bottom: -0.686em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;" class="mjx-line"></span></span><span style="height: 1.464em; vertical-align: -0.485em;" class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span><span class="mjx-msqrt MJXc-space3"><span class="mjx-box" style="padding-top: 0.045em;"><span class="mjx-surd"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">√</span></span><span class="mjx-box" style="padding-top: 0.13em; border-top: 1px solid;"><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span>. The document images are resized and then padded to achieve the desired input size. This input size allows us to use the Swin base model architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows" class="ltx_ref">24</a>]</cite>. We initialize the model with the pre-trained weights.
<br class="ltx_break">The Transformer decoder has a maximal sequence length of <span id="S3.SS1.p1.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="S=4096"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4096</span></span></span></span></span></span></span>. This relatively large sizing is due to the fact that the text of academic research papers can be dense and the syntax for tables in particular is token intensive. The BART decoder is a decoder-only transformer with 10 layers. The entire architecture has a total of 350M parameters.

<br class="ltx_break">We also test experiment with a smaller model (250M parameters) with a slightly smaller sequence length of <span id="S3.SS1.p1.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="S=3584"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3584</span></span></span></span></span></span></span> and only 4 decoder layers, where we start from the pre-trained base model. 
<br class="ltx_break">During inference the text is generated using greedy decoding.

<br class="ltx_break"><span class="ltx_text ltx_font_bold">Training</span>   We use an AdamW optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="Decoupled Weight Decay Regularization" class="ltx_ref">27</a>]</cite> to train for 3 epochs with an effective batch size of 192. Due to training instabilities, we choose a learning rate of <span id="S3.SS1.p1.m5" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathrm{lr}_{\rm init}=5\cdot 10^{-5}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">l</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">r</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">i</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">n</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">i</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">t</span></span></span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span></span></span></span></span></span></span></span></span> which is reduced by a factor of <span id="S3.SS1.p1.m6" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0.9996"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.9996</span></span></span></span></span></span></span> every 15 updates until it reaches <span id="S3.SS1.p1.m7" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathrm{lr}_{\rm end}=7.5\cdot 10^{-6}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">l</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">r</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">n</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">d</span></span></span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">7.5</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">6</span></span></span></span></span></span></span></span></span></span></span>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Augmentation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">In image recognition tasks, it is often beneficial to use data augmentation to improve generalization. Since we are only using digital-born academic research papers, we need to employ a number of transformations to simulate the imperfections and variability of scanned documents. These transformations include erosion, dilation, gaussian noise, gaussian blur, bitmap conversion, image compression, grid distortion and elastic transform <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="Best practices for convolutional neural networks applied to visual document analysis" class="ltx_ref">37</a>]</cite>. Each has a fixed probability of being applied to a given image. The transformations are implemented in the <em class="ltx_emph ltx_font_italic">Albumentations</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="Albumentations: Fast and Flexible Image Augmentations" class="ltx_ref">8</a>]</cite> library. For an overview of the effect of each transformation, see Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.2 Data Augmentation ‣ 3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.

<br class="ltx_break">During training time, we also add perturbations to the ground truth text by randomly replacing tokens. We found this to reduce the collapse into a repeating loop significantly. For more details, see Section <a href="#S5.SS4" title="5.4 Repetitions during inference ‣ 5 Results &amp; Evaluation ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4</span></a>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering" width="639" height="351" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" style="font-size:90%;">List of the different image augmentation methods used during training on an example snippet form a sample document.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Datasets</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">To the best of our knowledge there is no paired dataset of PDF pages and corresponding source code out there, so we created our own from the open access articles on arXiv.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a href="https://arxiv.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/</a></span></span></span> For layout diversity we also include a subset of the <em class="ltx_emph ltx_font_italic">PubMed Central</em> <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a href="https://www.ncbi.nlm.nih.gov/pmc/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.ncbi.nlm.nih.gov/pmc/</a></span></span></span> (PMC) open access non-commercial dataset. During the pretraining, a portion of the <em class="ltx_emph ltx_font_italic">Industry Documents Library</em> <span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a href="https://www.industrydocuments.ucsf.edu/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.industrydocuments.ucsf.edu/</a></span></span></span> (IDL) is included. See Table <a href="#A1.T1" title="Table A.1 ‣ Appendix A Dataset ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a> for the dataset composition.
</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">arXiv</span>   We collected the source code and compiled PDFs from 1,748,201 articles released on arXiv. To ensure consistent formatting, we first process the source files using <em class="ltx_emph ltx_font_italic">LaTeXML</em><span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a href="http://dlmf.nist.gov/LaTeXML/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dlmf.nist.gov/LaTeXML/</a></span></span></span> and convert them into HTML5 files. This step was important as it standardized and removed ambiguity from the LaTeX source code, especially in mathematical expressions. The conversion process included replacing user-defined macros, standardizing whitespace, adding optional brackets, normalizing tables, and replacing references and citations with their correct numbers.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p">We then parse the HTML files and convert them into a lightweight markup language that supports various elements such as headings, bold and italic text, algorithms, LaTeX inline and display math and LaTeX tables. This way, we ensure that the source code is properly formatted and ready for further processing.
<br class="ltx_break">The process is visualized in Fig. <a href="#S4.F3" title="Figure 3 ‣ 4 Datasets ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering" width="642" height="182" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" style="font-size:90%;">Data processing. The source file is converted into HTML which is then converted to Markdown. a) The LaTeX source provided by the authors. b) The HTML file computed form the LaTeX source using LaTeXML. c) The Markdown file parsed from the HTML file. d) The PDF file provided by the authors</span></figcaption>
</figure>
<div id="S4.p4" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">PMC</span>   We also processed articles from PMC, where XML files with semantic information are available in addition to the PDF file. We parse these files into the same markup language format as the arXiv articles. We chose to use far fewer articles from PMC because the XML files are not always as rich in semantic information. Often times equations and tables are stored as images and these cases are not trivial to detect, which leads to our decision to limit the use of PMC articles to the pre-training phase.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p class="ltx_p">The XML files are parsed into the same markup language as described above.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">IDL</span>   The IDL is a collection of documents produced by industries that have an impact on public health and is maintained by the University of California, San Francisco Library. Biten et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="OCR-IDL: OCR Annotations for Industry Document Library Dataset" class="ltx_ref">6</a>]</cite> provide high quality OCR text for PDFs from the IDL dataset. This does not include text formatting and is only used for pre-training to teach the model basic OCR of scanned documents.
<br class="ltx_break"></p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Splitting the pages</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">We split the markdown files according to the page breaks in the PDF file and rasterize each page as an image to create the final paired dataset.
During the compilation, the LaTeX compiler determines the page breaks of the PDF file automatically. Since we are not recompiling the LaTeX sources for each paper, we must heuristically split the source file into parts, which correspond to different pages. To achieve that we are using the embedded text on the PDF page and match it to source text.
<br class="ltx_break">However, figures and tables in the PDF may not correspond to their position in the source code.
To address this issue, we remove these elements in a pre-processing step using <code class="ltx_verbatim ltx_font_typewriter">pdffigures2</code> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="PDFFigures 2.0: Mining Figures from Research Papers" class="ltx_ref">9</a>]</cite>. The recognized captions are are then compared to the captions in the XML file and matched based on their Levenshtein distance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="Binary codes capable of correcting deletions, insertions, and reversals" class="ltx_ref">21</a>]</cite>. Once the source document has been split into individual pages, the removed figures and tables are reinserted at the end of each page. 
<br class="ltx_break">For a better matching we also replaced unicode characters in the PDF text with corresponding LaTeX commands using the pylatexenc-library<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a href="https://github.com/phfaist/pylatexenc" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/phfaist/pylatexenc</a></span></span></span>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Bag of Words matching</span>  
First we extract the text lines from the PDF using MuPDF<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a href="https://mupdf.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://mupdf.com/</a></span></span></span> and preprocess them to remove page numbers and potential headers/footers. We then use a <em class="ltx_emph ltx_font_italic">Bag of Words</em> model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="Distributional Structure" class="ltx_ref">15</a>]</cite> with TF-IDF vectorizer and a linear Support Vector Machine classifier. The model is fitted to the PDF lines with the page number as label. Next we split the LaTeX source into paragraphs and predict the page number for each of them.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p class="ltx_p">Ideally, the predictions will form a stair case function but in practice the signal will be noisy. To find the best boundary points we employ a similar logic as decision trees and minimize a measure based on the <em class="ltx_emph ltx_font_italic">Gini</em> impurity</p>
<div class="ltx_engrafo_equation_container"><table id="S4.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S4.Ex1.m1" class="ltx_DisplayMath"><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: left;"><span class="mjx-math" aria-label=" G_{[a,\>\!b]}(i)=(b-a)\cdot\left(1-p_{[a,\>\!b]}^{2}(i)-p_{[a,\>\!b]}^{2}(i+1)%
\right), "><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">G</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow" style="width: 1.871em;"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="font-size: 141.4%; width: 0.222em; height: 0px;"></span><span class="mjx-mspace" style="font-size: 141.4%; margin-right: -0.167em; width: 0px; height: 0px;"></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span><span class="mjx-mrow MJXc-space2"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.961em; padding-bottom: 0.961em;">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span><span class="mjx-stack" style="vertical-align: -0.374em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow" style="width: 1.871em;"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="font-size: 141.4%; width: 0.222em; height: 0px;"></span><span class="mjx-mspace" style="font-size: 141.4%; margin-right: -0.167em; width: 0px; height: 0px;"></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span><span class="mjx-stack" style="vertical-align: -0.374em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow" style="width: 1.871em;"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="font-size: 141.4%; width: 0.222em; height: 0px;"></span><span class="mjx-mspace" style="font-size: 141.4%; margin-right: -0.167em; width: 0px; height: 0px;"></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.961em; padding-bottom: 0.961em;">)</span></span></span><span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span></span></span></span></span></span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody></table></div>
<p class="ltx_p">where <span id="S4.SS1.p3.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p_{[a,\>\!b]}(i)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow" style="width: 1.871em;"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="font-size: 141.4%; width: 0.222em; height: 0px;"></span><span class="mjx-mspace" style="font-size: 141.4%; margin-right: -0.167em; width: 0px; height: 0px;"></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> is the probability of choosing an element with the predicted page number <span id="S4.SS1.p3.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="i"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span></span> in the interval <span id="S4.SS1.p3.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="[a,\,b]"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 0.167em; height: 0px;"></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span></span> that describes which paragraphs (elements) were considered for the split.
<br class="ltx_break">The best splitting position <span id="S4.SS1.p3.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span> in the interval <span id="S4.SS1.p3.m5" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="[a,\,b]"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 0.167em; height: 0px;"></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span></span> is then</p>
<div class="ltx_engrafo_equation_container"><table id="S4.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S4.Ex2.m1" class="ltx_DisplayMath"><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: left;"><span class="mjx-math" aria-label=" {\hat{t}}_{i}=\operatorname*{\arg\,\min}_{t}\left(G_{[a,\>\!t]}(i)+G_{[t,\>\!b%
]}(i)\right). "><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.014em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span><span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-munderover MJXc-space3"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">arg</span></span><span class="mjx-mspace" style="width: 0.167em; height: 0px;"></span><span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">min</span></span></span></span></span></span></span><span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 2.218em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.593em; padding-bottom: 0.593em;">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">G</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow" style="width: 1.803em;"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="font-size: 141.4%; width: 0.222em; height: 0px;"></span><span class="mjx-mspace" style="font-size: 141.4%; margin-right: -0.167em; width: 0px; height: 0px;"></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">G</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow" style="width: 1.703em;"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="font-size: 141.4%; width: 0.222em; height: 0px;"></span><span class="mjx-mspace" style="font-size: 141.4%; margin-right: -0.167em; width: 0px; height: 0px;"></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.593em; padding-bottom: 0.593em;">)</span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">.</span></span></span></span></span></span></span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody></table></div>
<p class="ltx_p">The search process starts with all paragraphs and for each subsequent page break, the lower bound of the search interval is set to the previous split position. See Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.1 Splitting the pages ‣ 4 Datasets ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> for a visualization of an example page.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering" width="339" height="212" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" style="font-size:90%;">Example for splitting the paragraphs in the source code into different pages. The points in blue denote the page index predicted by the SVM. </span></figcaption>
</figure>
<div id="S4.SS1.p4" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Fuzzy matching</span>  
After this first coarse document splitting we try to find the exact position within the paragraph. This is done by comparing the source text within the neighborhood of the predicted splitting position to the last sentences of the previous page of the embedded PDF text, and the first sentences of the next page using the <code class="ltx_verbatim ltx_font_typewriter">fuzzysearch</code> library<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a href="https://github.com/taleinat/fuzzysearch" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/taleinat/fuzzysearch</a></span></span></span>.
If the two dividing points are at the same location in the source text, the page break is considered “accurate” and receives a score of 1. On the other hand, if the splitting positions differ, the one with the smallest normalized Levenshtein distance is selected and given a score of 1 minus the distance. To be included in the dataset, a PDF page must have an average score of at least 0.9 for both page breaks. This results in an acceptance rate of about <span id="S4.SS1.p4.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="47\%"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">47</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">%</span></span></span></span></span></span></span> of all pages.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ground truth artifacts</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">Because the dataset was pre-processed by LaTeXML, the markup version of the source code can contain artifacts and commands from unsupported packages. The HTML file may contain subsection titles with numbering even though they are not numbered in the PDF. There may also be instances where figures or tables are missing from the ground truth due to processing errors.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p">In addition, the splitting algorithm of the source code will in some cases include text from the previous page or cut off words from the end. This is especially true for “invisible” characters used for formatting, like italic, bold text or section header.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p class="ltx_p">For PMC papers the inline math is written as Unicode or italic text, while display math equations or tables are often included in image format and will therefore be ignored.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p class="ltx_p">Each of these issues reduces the overall data quality. However, the large number of training samples compensates for these small errors.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results &amp; Evaluation</h2>

<figure id="S5.F5" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subfigure">
<figure id="S5.F5.fig1" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="figures/scalinglaw15.png" id="S5.F5.g1" class="ltx_graphics" width="541" height="699" alt=""></span></p>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="S5.F5.fig2" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="x5.png" id="S5.F5.g2" class="ltx_graphics" width="675" height="874" alt=""></span></p>
</figure>
</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" style="font-size:90%;">Example of a page with many mathematical equations taken from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="Beyond neural scaling laws: beating power law scaling via data pruning" class="ltx_ref">40</a>]</cite>. Left: Image of a page in the document, Right: Model output converted to LaTeX and rendered to back into a PDF. Examples of scanned documents can be found in the appendix <a href="#A2" title="Appendix B Examples ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</span></figcaption>
</figure>
<div id="S5.p1" class="ltx_para">
<p class="ltx_p">In this section we discuss the results and performance of the model. For an example see Fig. <a href="#S5.F5" title="Figure 5 ‣ 5 Results &amp; Evaluation ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> or go to Sec. <a href="#A2" title="Appendix B Examples ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>. The model focuses only on the important content relevant features of the page. The box around the equations is skipped.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Metrics</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p">We report the following metrics on our test set.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Edit distance</span>  
The edit distance, or Levenshtein distance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="Binary codes capable of correcting deletions, insertions, and reversals" class="ltx_ref">21</a>]</cite>, measures the number of character manipulations (insertions, deletions, substitutions) it takes to get from one string to another. In this work we consider the normalized edit distance, where we divide by the total number of characters.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">BLEU</span>  
The BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="Bleu: a Method for Automatic Evaluation of Machine Translation" class="ltx_ref">34</a>]</cite> metric was originally introduced for measuring the quality of text that has been machine-translated from one language to another. The metric computes a score based on the number of matching n-grams between the candidate and reference sentence.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">METEOR</span>   Another machine-translating metric with a focus on recall instead of precision, introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">F-measure</span>  
We also compute the F1-score and report the precision and recall.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Text modalities</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p">In a scientific research article, there are three distinct types of text: 1) plain text, which comprises the majority of the document, 2) mathematical expressions, and 3) tables. It is important to separately examine each of these components during the evaluation process. This is necessary because in LaTeX, there are multiple ways to express the same mathematical expression. While some variability has been eliminated during the LaTeXML pre-processing step, there still is a significant amount of ambiguity present, like ordering of subscript and superscript, equivalent commands with different notation (<code class="ltx_verbatim ltx_font_typewriter">stackrel</code>, <code class="ltx_verbatim ltx_font_typewriter">atop</code>, <code class="ltx_verbatim ltx_font_typewriter">substack</code> or <code class="ltx_verbatim ltx_font_typewriter">frac</code>, <code class="ltx_verbatim ltx_font_typewriter">over</code>), situationally interchangeable commands (<code class="ltx_verbatim ltx_font_typewriter">bm</code>, <code class="ltx_verbatim ltx_font_typewriter">mathbf</code>, <code class="ltx_verbatim ltx_font_typewriter">boldsymbol</code>, <code class="ltx_verbatim ltx_font_typewriter">bf</code> or <code class="ltx_verbatim ltx_font_typewriter">\left(</code>, <code class="ltx_verbatim ltx_font_typewriter">\big(</code>, etc.), whitespace commands, additional layers of brackets, and more. As a consequence, there can be a discrepancy between prediction and ground truth, even if the rendered formulas appear identical. 
<br class="ltx_break">In addition, it is not always possible to determine, where a inline math environment ends and text begins, when writing numbers and punctuation (Example: <code class="ltx_verbatim ltx_font_typewriter">$\mathrm{H}_{0}$1,</code> vs. <code class="ltx_verbatim ltx_font_typewriter">H$_{0}1,$</code> <span id="S5.SS2.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\to"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span></span></span></span></span></span> <span id="S5.SS2.p1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathrm{H}_{0}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">H</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span></span></span>1, vs. H<span id="S5.SS2.p1.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}_{0}1,"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span></span></span></span></span></span>). This ambiguity reduces both math and plain text scores.
<br class="ltx_break">The expected score for mathematical expressions is lower than for plain text.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Comparison</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p class="ltx_p">We present our results in Table <a href="#S5.T1" title="Table 1 ‣ 5.3 Comparison ‣ 5 Results &amp; Evaluation ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
As expected, the mathematical expressions have the worst agreement with the ground truth.
For the plain text, most discrepancies come from formatting ambiguities and missing text due to inline math, as described above.
The output format of GROBID is an XML file, which we convert into a compatible markup language, similar to the PMC or arXiv files.
To some extent, GROBID provides support for formulas in its output, but it identifies and stores them as the Unicode representations embedded in the PDF. We replace each Unicode symbol with its corresponding LaTeX command to increase the similarity. Additionally, GROBID mislabels small inline expressions as text. For identified formulas, GROBID stores the bounding box coordinates. We modify the program by sending the snippet to the external formula recognition software LaTeX-OCR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="Pix2tex - LaTeX OCR" class="ltx_ref">7</a>]</cite>. This way we can also get a signal for math modality. The reported results in this section are quite poor, primarily due to the amount of missed formulas by GROBID and the equation prediction accuracy is affected by the quality of the bounding boxes. The performance of the embedded PDF text alone is better than GROBID, which is due to formatting differences for the title page or reference section.

<br class="ltx_break">Both Nougat small and base are able to outperform the other approach and achieve high scores in all metrics. We note that the performance of the smaller model is on par with the larger base model.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">Method</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Modality</th>
<td class="ltx_td ltx_align_center ltx_border_tt">Edit distance <span id="S5.T1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\downarrow"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">↓</span></span></span></span></span></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt">BLEU <span id="S5.T1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\uparrow"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">↑</span></span></span></span></span></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt">METEOR <span id="S5.T1.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\uparrow"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">↑</span></span></span></span></span></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt">Precision <span id="S5.T1.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\uparrow"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">↑</span></span></span></span></span></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt">Recall <span id="S5.T1.m5" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\uparrow"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">↑</span></span></span></span></span></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt">F1 <span id="S5.T1.m6" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\uparrow"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">↑</span></span></span></span></span></span></span>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">PDF</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">All</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.255</td>
<td class="ltx_td ltx_align_center ltx_border_t">65.8</td>
<td class="ltx_td ltx_align_center ltx_border_t">82.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">77.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">81.4</td>
<td class="ltx_td ltx_align_center ltx_border_t">79.2</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">GROBID</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">All</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.312</td>
<td class="ltx_td ltx_align_center ltx_border_t">55.6</td>
<td class="ltx_td ltx_align_center ltx_border_t">71.9</td>
<td class="ltx_td ltx_align_center ltx_border_t">74.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">72.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">73.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Tables</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.626</td>
<td class="ltx_td ltx_align_center ltx_border_t">25.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">64.5</td>
<td class="ltx_td ltx_align_center ltx_border_t">61.4</td>
<td class="ltx_td ltx_align_center ltx_border_t">80.7</td>
<td class="ltx_td ltx_align_center ltx_border_t">69.7</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">+ LaTeX OCR</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Plain text</th>
<td class="ltx_td ltx_align_center">0.363</td>
<td class="ltx_td ltx_align_center">57.4</td>
<td class="ltx_td ltx_align_center">69.2</td>
<td class="ltx_td ltx_align_center">82.1</td>
<td class="ltx_td ltx_align_center">70.5</td>
<td class="ltx_td ltx_align_center">75.9</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Math</th>
<td class="ltx_td ltx_align_center">0.727</td>
<td class="ltx_td ltx_align_center">0.3</td>
<td class="ltx_td ltx_align_center">5.0</td>
<td class="ltx_td ltx_align_center">11.0</td>
<td class="ltx_td ltx_align_center">8.6</td>
<td class="ltx_td ltx_align_center">9.7</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="4">Nougat small (250M<span id="S5.T1.m7" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\ast}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span></span></span></span></span></span></span></span></span></span>)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">All</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.073</td>
<td class="ltx_td ltx_align_center ltx_border_t">88.9</td>
<td class="ltx_td ltx_align_center ltx_border_t">92.8</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">93.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">92.2</td>
<td class="ltx_td ltx_align_center ltx_border_t">92.9</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Tables</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.220</td>
<td class="ltx_td ltx_align_center ltx_border_t">68.5</td>
<td class="ltx_td ltx_align_center ltx_border_t">78.6</td>
<td class="ltx_td ltx_align_center ltx_border_t">75.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">79.8</td>
<td class="ltx_td ltx_align_center ltx_border_t">77.3</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Plain text</th>
<td class="ltx_td ltx_align_center">0.058</td>
<td class="ltx_td ltx_align_center">91.0</td>
<td class="ltx_td ltx_align_center">94.3</td>
<td class="ltx_td ltx_align_center">96.1</td>
<td class="ltx_td ltx_align_center">95.3</td>
<td class="ltx_td ltx_align_center">95.7</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Math</th>
<td class="ltx_td ltx_align_center">0.117</td>
<td class="ltx_td ltx_align_center">56.0</td>
<td class="ltx_td ltx_align_center">74.7</td>
<td class="ltx_td ltx_align_center">77.1</td>
<td class="ltx_td ltx_align_center">76.8</td>
<td class="ltx_td ltx_align_center">76.9</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" rowspan="4">Nougat base (350M<span id="S5.T1.m8" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\ast}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span></span></span></span></span></span></span></span></span></span>)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">All</th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">0.071</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">89.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">93.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">93.5</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">92.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">93.1</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Tables</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.211</td>
<td class="ltx_td ltx_align_center ltx_border_t">69.7</td>
<td class="ltx_td ltx_align_center ltx_border_t">79.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">75.4</td>
<td class="ltx_td ltx_align_center ltx_border_t">80.7</td>
<td class="ltx_td ltx_align_center ltx_border_t">78.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Plain text</th>
<td class="ltx_td ltx_align_center">0.058</td>
<td class="ltx_td ltx_align_center">91.2</td>
<td class="ltx_td ltx_align_center">94.6</td>
<td class="ltx_td ltx_align_center">96.2</td>
<td class="ltx_td ltx_align_center">95.3</td>
<td class="ltx_td ltx_align_center">95.7</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Math</th>
<td class="ltx_td ltx_align_center ltx_border_bb">0.128</td>
<td class="ltx_td ltx_align_center ltx_border_bb">56.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb">75.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb">76.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb">76.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb">76.5</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" style="font-size:90%;">Results on arXiv test set. PDF is the text embedded in the PDF file. The modality “All” refers to the output text without any splitting. <span id="S5.T1.m10" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\ast}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span></span></span></span></span></span></span></span></span></span>Number of parameters.</span></figcaption>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Repetitions during inference</h3>

<figure id="S5.F6" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subfigure">
<figure id="S5.F6.fig1" class="ltx_figure ltx_align_center"><img src="x6.png" id="S5.F6.g1" class="ltx_graphics" width="676" height="203" alt="">
</figure>
</td>
<td class="ltx_subfigure">
<figure id="S5.F6.fig2" class="ltx_figure ltx_align_center"><img src="x7.png" id="S5.F6.g2" class="ltx_graphics" width="676" height="203" alt="">
</figure>
</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" style="font-size:90%;">Examples for repetition detection on logits. Top: Sample with repetition, Bottom: Sample without repetition. Left: Highest logit score for each token in the sequence <span id="S5.F6.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\ell(x)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>, Center: Sliding window variance of the logits <span id="S5.F6.m5" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\operatorname{VarWin}_{B}[\ell](x)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">VarWin</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>, Right: Variance of variance from the position to the end <span id="S5.F6.m6" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\operatorname{VarEnd}_{B}[\ell](x)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">VarEnd</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></span></figcaption>
</figure>
<div id="S5.SS4.p1" class="ltx_para">
<p class="ltx_p">We notice that the model degenerates into repeating the same sentence over and over again. The model can not recover from this state by itself. In its simplest form, the last sentence or paragraph is repeated over and over again. We observed this behavior in <span id="S5.SS4.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1.5\%"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1.5</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">%</span></span></span></span></span></span></span> of pages in the test set, but the frequency increases for out-of-domain documents. Getting stuck in a repetitive loop is a known problem with Transformer-based models, when sampled with greedy decoding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="The Curious Case of Neural Text Degeneration" class="ltx_ref">16</a>]</cite>.
<br class="ltx_break">It can also happen that the model alternates between two sentences but sometimes changes some words, so a strict repetition detection will not suffice. Even harder to detect are predictions where the model counts its own repetitions, which sometimes happens in the references section.
<br class="ltx_break">In general we notice this kind behavior after a mistake by the model. The model is not able to recover from the collapse.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Anti-repetition augmentation</span>  
Because of that we introduce a random perturbation during training. This helps the model to learn how to handle a wrongly predicted token.
For each training example, there is a fixed probability that a random token will be replaced by any other randomly chosen token. This process continues until the newly sampled number is greater than a specified threshold (in this case, 10%). We did not observe a decrease in performance with this approach, but we did notice a significant reduction in repetitions. Particularly for out-of-domain documents, where we saw a 32% decline in failed page conversions.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Repetition detection</span>  
Since we are generating a maximum of <span id="S5.SS4.p3.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="4096"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4096</span></span></span></span></span></span></span> tokens the model will stop at some point, however it is very inefficient and resource intensive to wait for a “end of sentence” token, when none will come. To detect the repetition during inference time we look at the largest logit value <span id="S5.SS4.p3.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\ell_{i}=\max\boldsymbol{\ell}_{i}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">max</span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">ℓ</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span></span></span></span></span></span> of the ith token. We found that the logits after a collapse can be separated using the following heuristic. First calculate the variance of the logits for a sliding window of size <span id="S5.SS4.p3.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="B=15"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">15</span></span></span></span></span></span></span></p>
<div class="ltx_engrafo_equation_container"><table id="S5.Ex3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S5.Ex3.m1" class="ltx_DisplayMath"><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: left;"><span class="mjx-math" aria-label=" \operatorname{VarWin}_{B}[\boldsymbol{\ell}](x)=\frac{1}{B}\sum_{i=x}^{x+B}%
\left(\ell_{i}-\frac{1}{B}\sum_{j=x}^{x+B}\ell_{j}\right)^{2}. "><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">VarWin</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">ℓ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.959em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 0.959em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span><span class="mjx-denominator" style="width: 0.959em; bottom: -0.773em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.959em;" class="mjx-line"></span></span><span style="height: 2.141em; vertical-align: -0.773em;" class="mjx-vsize"></span></span><span class="mjx-munderover MJXc-space1"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; padding-bottom: 0.176em; padding-top: 0.141em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span><span class="mjx-op" style="padding-left: 0.024em;"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.74em; padding-bottom: 0.74em;">∑</span></span></span></span></span></span><span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.207em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 1.551em; padding-bottom: 1.551em;">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 0.959em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 0.959em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span><span class="mjx-denominator" style="width: 0.959em; bottom: -0.773em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.959em;" class="mjx-line"></span></span><span style="height: 2.141em; vertical-align: -0.773em;" class="mjx-vsize"></span></span><span class="mjx-munderover MJXc-space1"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; padding-bottom: 0.176em; padding-top: 0.141em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span><span class="mjx-op" style="padding-left: 0.024em;"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.74em; padding-bottom: 0.74em;">∑</span></span></span></span></span></span><span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.174em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">j</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">j</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 1.551em; padding-bottom: 1.551em;">)</span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 2.125em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">.</span></span></span></span></span></span></span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody></table></div>
<p class="ltx_p">Here <span id="S5.SS4.p3.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\ell"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span></span></span></span></span></span> is the signal of logits and <span id="S5.SS4.p3.m5" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> the index. Using this new signal we compute variances again but this time from the point <span id="S5.SS4.p3.m6" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> to the end of the sequence</p>
<div class="ltx_engrafo_equation_container"><table id="S5.Ex4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S5.Ex4.m1" class="ltx_DisplayMath"><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: left;"><span class="mjx-math" aria-label=" \operatorname{VarEnd}_{B}[\boldsymbol{\ell}](x)=\frac{1}{S-x}\sum_{i=x}^{S}%
\left(\operatorname{VarWin}_{B}[\boldsymbol{\ell}](i)-\frac{1}{S-x}\sum_{j=x}^%
{S}\operatorname{VarWin}_{B}[\boldsymbol{\ell}](i)\right)^{2}. "><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">VarEnd</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">ℓ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 2.639em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 2.639em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span><span class="mjx-denominator" style="width: 2.639em; bottom: -0.877em;"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 2.639em;" class="mjx-line"></span></span><span style="height: 2.245em; vertical-align: -0.877em;" class="mjx-vsize"></span></span><span class="mjx-munderover MJXc-space1"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; padding-bottom: 0.236em; padding-top: 0.141em; padding-left: 0.699em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span></span></span></span><span class="mjx-op"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.74em; padding-bottom: 0.74em;">∑</span></span></span></span></span></span><span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.174em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 1.551em; padding-bottom: 1.551em;">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">VarWin</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">ℓ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 2.639em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 2.639em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span><span class="mjx-denominator" style="width: 2.639em; bottom: -0.877em;"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 2.639em;" class="mjx-line"></span></span><span style="height: 2.245em; vertical-align: -0.877em;" class="mjx-vsize"></span></span><span class="mjx-munderover MJXc-space1"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; padding-bottom: 0.236em; padding-top: 0.141em; padding-left: 0.699em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span></span></span></span><span class="mjx-op"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.74em; padding-bottom: 0.74em;">∑</span></span></span></span></span></span><span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.14em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">j</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">VarWin</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">ℓ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 1.551em; padding-bottom: 1.551em;">)</span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 2.147em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">.</span></span></span></span></span></span></span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody></table></div>
<p class="ltx_p">If this signal drops below a certain threshold (we choose 6.75) and stays below for the remainder of the sequence, we classify the sequence to have repetitions.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p class="ltx_p">During inference time, it is obviously not possible to compute the to the end of the sequence if our goal is to stop generation at an earlier point in time. So here we work with a subset of the last 200 tokens and a half the threshold. After the generation is finished, the procedure as described above is repeated for the full sequence.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Limitations &amp; Future work</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Utility</span>  
The utility of the model is limited by a number of factors. First, the problem with repetitions outlined in section <a href="#S5.SS4" title="5.4 Repetitions during inference ‣ 5 Results &amp; Evaluation ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4</span></a>.
The model is trained on research papers, which means it works particularly well on documents with a similar structure. However, it can still accurately convert other types of documents. 
<br class="ltx_break">Nearly every dataset sample is in English. Initial tests on a small sample suggest that the model’s performance with other Latin-based languages is satisfactory, although any special characters from these languages will be replaced with the closest equivalent from the Latin alphabet. Non-Latin script languages result in instant repetitions.

<br class="ltx_break"><span class="ltx_text ltx_font_bold">Generation Speed</span>  
On a machine with a NVIDIA A10G graphics card with 24GB VRAM we can process 6 pages in parallel. The generation speed depends heavily on the amount of text on any given page. With an average number of tokens of <span id="S5.SS5.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\approx 1400"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">≈</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1400</span></span></span></span></span></span></span> we get an mean generation time of 19.5s per batch for the base model without any inference optimization. Compared to classical approaches (GROBID 10.6 PDF/s <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="GROBID" class="ltx_ref">26</a>]</cite>) this is very slow, but it is not limited to digital-born PDFs and can correctly parse mathematical expressions.

<br class="ltx_break"><span class="ltx_text ltx_font_bold">Future work</span>  
The model is trained on one page at a time without knowledge about other pages in the document. This results in inconsistencies across the document. Most notably in the bibliography where the model was trained on different styles or section titles where sometimes numbers are skipped or hallucinated. Though handling each page separately significantly improves parallelization and scalability, it may diminish the quality of the merged document text.

<br class="ltx_break">The primary challenge to solve is the tendency for the model to collapse into a repeating loop, which is left for future work.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">In this work, we present Nougat, an end-to-end trainable encoder-decoder transformer based model for converting document pages to markup. We apply recent advances in visual document understanding to a novel OCR task. Distinct from related approaches, our method does not rely on OCR or embedded text representations, instead relying solely on the rasterized document page.
Moreover, we have illustrated an automatic and unsupervised dataset generation process that we used to successfully train the model for scientific document to markup conversion. Overall, our approach has shown great potential for not only extracting text from digital-born PDFs but also for converting scanned papers and textbooks. We hope this work can be a starting point for future research in related domains.

<br class="ltx_break">All the code for model evaluation, training and dataset generation can be accessed at <a href="https://github.com/facebookresearch/nougat" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/nougat</a>.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgments</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p">Thanks to Ross Taylor, Marcin Kardas, Iliyan Zarov, Kevin Stone, Jian Xiang Kuan, Andrew Poulton and Hugo Touvron for their valuable discussions and feedback.

<br class="ltx_break">Thanks to Faisal Azhar for the support throughout the project.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="bib.L1" class="ltx_biblist">
<li id="bib.bib23" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. Álvaro, J. Sánchez, and J. Benedí</span><span class="ltx_text ltx_bib_year"> (2014-01)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Recognition of on-line handwritten mathematical expressions using 2D stochastic context-free grammars and hidden Markov models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Pattern Recognition Letters</span> <span class="ltx_text ltx_bib_volume">35</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;58–67</span> (<span class="ltx_text ltx_bib_language">en</span>).
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0167-8655</span>,
<a href="https://www.sciencedirect.com/science/article/pii/S016786551200308X" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1016/j.patrec.2012.09.023" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Appalaraju, B. Jasani, B. U. Kota, Y. Xie, and R. Manmatha</span><span class="ltx_text ltx_bib_year"> (2021-09)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">DocFormer: End-to-End Transformer for Document Understanding</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:2106.11539 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/2106.11539" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.48550/arXiv.2106.11539" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p1" title="3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Awal, H. Mouchre, and C. Viard-Gaudin</span><span class="ltx_text ltx_bib_year"> (2014-01)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A global learning approach for an online handwritten mathematical expression recognition system</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Pattern Recognition Letters</span> <span class="ltx_text ltx_bib_volume">35</span> (<span class="ltx_text ltx_bib_number">C</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;68–77</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0167-8655</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Banerjee and A. Lavie</span><span class="ltx_text ltx_bib_year"> (2005-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Ann Arbor, Michigan</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;65–72</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://aclanthology.org/W05-0909" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.p4" title="5.1 Metrics ‣ 5 Results &amp; Evaluation ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1</span></a>.
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Bautista and R. Atienza</span><span class="ltx_text ltx_bib_year"> (2022-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Scene Text Recognition with Permuted Autoregressive Sequence Models</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:2207.06966 [cs]
version: 1</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/2207.06966" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. F. Biten, R. Tito, L. Gomez, E. Valveny, and D. Karatzas</span><span class="ltx_text ltx_bib_year"> (2022-02)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">OCR-IDL: OCR Annotations for Industry Document Library Dataset</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:2202.12985 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/2202.12985" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.48550/arXiv.2202.12985" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p6" title="4 Datasets ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Blecher</span><span class="ltx_text ltx_bib_year"> (2023-02)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Pix2tex - LaTeX OCR</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">original-date: 2020-12-11T16:35:13Z</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://github.com/lukas-blecher/LaTeX-OCR" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S5.SS3.p1" title="5.3 Comparison ‣ 5 Results &amp; Evaluation ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.3</span></a>.
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Buslaev, V. I. Iglovikov, E. Khvedchenya, A. Parinov, M. Druzhinin, and A. A. Kalinin</span><span class="ltx_text ltx_bib_year"> (2020-02)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Albumentations: Fast and Flexible Image Augmentations</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Information</span> <span class="ltx_text ltx_bib_volume">11</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;125</span> (<span class="ltx_text ltx_bib_language">en</span>).
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 2078-2489</span>,
<a href="https://www.mdpi.com/2078-2489/11/2/125" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3390/info11020125" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.p1" title="3.2 Data Augmentation ‣ 3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2</span></a>.
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Clark and S. Divvala</span><span class="ltx_text ltx_bib_year"> (2016-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">PDFFigures 2.0: Mining Figures from Research Papers</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Newark New Jersey USA</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;143–152</span> (<span class="ltx_text ltx_bib_language">en</span>).
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-4503-4229-2</span>,
<a href="https://dl.acm.org/doi/10.1145/2910896.2910904" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1145/2910896.2910904" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.p1" title="4.1 Splitting the pages ‣ 4 Datasets ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Davis, B. Morse, B. Price, C. Tensmeyer, C. Wigington, and V. Morariu</span><span class="ltx_text ltx_bib_year"> (2022-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">End-to-end Document Recognition and Understanding with Dessurt</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:2203.16618 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/2203.16618" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p1" title="3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Deng, A. Kanervisto, J. Ling, and A. M. Rush</span><span class="ltx_text ltx_bib_year"> (2016-09)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Image-to-Markup Generation with Coarse-to-Fine Attention</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:1609.04938 [cs]
version: 1</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1609.04938" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.48550/arXiv.1609.04938" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. H. Diaz, S. Qin, R. Ingle, Y. Fujii, and A. Bissacco</span><span class="ltx_text ltx_bib_year"> (2021-04)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Rethinking Text Line Recognition Models</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:2104.07787 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/2104.07787" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby</span><span class="ltx_text ltx_bib_year"> (2021-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:2010.11929 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/2010.11929" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.48550/arXiv.2010.11929" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p3" title="3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Fan, M. Lewis, and Y. Dauphin</span><span class="ltx_text ltx_bib_year"> (2018-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Hierarchical Neural Story Generation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Melbourne, Australia</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;889–898</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://aclanthology.org/P18-1082" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.18653/v1/P18-1082" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A2.F4" title="Figure B.4 ‣ Appendix B Examples ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure B.4</span></a>.
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. S. Harris</span><span class="ltx_text ltx_bib_year"> (1954)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Distributional Structure</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal"><span class="ltx_text ltx_font_italic">WORD</span></span> <span class="ltx_text ltx_bib_volume">10</span> (<span class="ltx_text ltx_bib_number">2-3</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;146–162</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note"><span>Publisher: Routledge
_eprint: <a href="https://doi.org/10.1080/00437956.1954.11659520">https://doi.org/10.1080/00437956.1954.11659520</a></span></span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://doi.org/10.1080/00437956.1954.11659520" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1080/00437956.1954.11659520" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.p2" title="4.1 Splitting the pages ‣ 4 Datasets ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi</span><span class="ltx_text ltx_bib_year"> (2020-02)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The Curious Case of Neural Text Degeneration</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:1904.09751 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1904.09751" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS4.p1" title="5.4 Repetitions during inference ‣ 5 Results &amp; Evaluation ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.4</span></a>.
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Huang, T. Lv, L. Cui, Y. Lu, and F. Wei</span><span class="ltx_text ltx_bib_year"> (2022-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:2204.08387 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/2204.08387" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Kim, T. Hong, M. Yim, J. Nam, J. Park, J. Yim, W. Hwang, S. Yun, D. Han, and S. Park</span><span class="ltx_text ltx_bib_year"> (2022-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">OCR-free Document Understanding Transformer</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:2111.15664 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/2111.15664" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.48550/arXiv.2111.15664" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.F1" title="Figure 1 ‣ 3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>,
<a href="#S3.p1" title="3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>,
<a href="#S3.p2" title="3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>,
<a href="#S3.p4" title="3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_year"> (1970-01)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Kinetics and Thermodynamics in High-Temperature Gases</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">NTRS Report/Patent Number: N70-32106-116
NTRS Document ID: 19700022795
NTRS Research Center: Glenn Research Center (GRC)</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://ntrs.nasa.gov/citations/19700022795" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A2.F2" title="Figure B.2 ‣ Appendix B Examples ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure B.2</span></a>.
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. D. Le and M. Nakagawa</span><span class="ltx_text ltx_bib_year"> (2017-11)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Training an End-to-End System for Handwritten Mathematical Expression Recognition by Generated Patterns</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">01</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;1056–1061</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">ISSN: 2379-2140</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1109/ICDAR.2017.175" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">V. Levenshtein</span><span class="ltx_text ltx_bib_year"> (1965)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Binary codes capable of correcting deletions, insertions, and reversals</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Soviet physics. Doklady</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://www.semanticscholar.org/paper/Binary-codes-capable-of-correcting-deletions%2C-and-Levenshtein/b2f8876482c97e804bb50a5e2433881ae31d0cdd" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.p1" title="4.1 Splitting the pages ‣ 4 Datasets ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.1</span></a>,
<a href="#S5.SS1.p2" title="5.1 Metrics ‣ 5 Results &amp; Evaluation ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1</span></a>.
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer</span><span class="ltx_text ltx_bib_year"> (2019-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:1910.13461 [cs, stat]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1910.13461" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.48550/arXiv.1910.13461" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p4" title="3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Li, T. Lv, J. Chen, L. Cui, Y. Lu, D. Florencio, C. Zhang, Z. Li, and F. Wei</span><span class="ltx_text ltx_bib_year"> (2022-09)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:2109.10282 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/2109.10282" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.48550/arXiv.2109.10282" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib1" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo</span><span class="ltx_text ltx_bib_year"> (2021-08)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:2103.14030 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/2103.14030" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.48550/arXiv.2103.14030" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p1" title="3.1 Setup ‣ 3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>,
<a href="#S3.p3" title="3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Lo, L. L. Wang, M. Neumann, R. Kinney, and D. Weld</span><span class="ltx_text ltx_bib_year"> (2020-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">S2ORC: The Semantic Scholar Open Research Corpus</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Online</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;4969–4983</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://aclanthology.org/2020.acl-main.447" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.18653/v1/2020.acl-main.447" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Lopez</span><span class="ltx_text ltx_bib_year"> (2023-02)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">GROBID</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">original-date: 2012-09-13T15:48:54Z</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://github.com/kermitt2/grobid" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p3" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S5.SS5.p1" title="5.5 Limitations &amp; Future work ‣ 5 Results &amp; Evaluation ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.5</span></a>.
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">I. Loshchilov and F. Hutter</span><span class="ltx_text ltx_bib_year"> (2019-01)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Decoupled Weight Decay Regularization</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:1711.05101 [cs, math]
version: 3</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1711.05101" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p1" title="3.1 Setup ‣ 3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>.
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Lu Wang and Wanmin Liu</span><span class="ltx_text ltx_bib_year"> (2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Online publishing via pdf2htmlEX</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">TUGboat</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://www.tug.org/TUGboat/tb34-3/tb108wang.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p3" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. MacLean and G. Labahn</span><span class="ltx_text ltx_bib_year"> (2013-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A new approach for recognizing handwritten mathematics using relational grammars and fuzzy sets</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">International Journal on Document Analysis and Recognition (IJDAR)</span> <span class="ltx_text ltx_bib_volume">16</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;139–163</span> (<span class="ltx_text ltx_bib_language">en</span>).
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 1433-2825</span>,
<a href="https://doi.org/10.1007/s10032-012-0184-x" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1007/s10032-012-0184-x" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Mahdavi, R. Zanibbi, H. Mouchere, C. Viard-Gaudin, and U. Garain</span><span class="ltx_text ltx_bib_year"> (2019-09)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">ICDAR 2019 CROHME + TFD: Competition on Recognition of Handwritten Mathematical Expressions and Typeset Formula Detection</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2019 International Conference on Document Analysis and Recognition (ICDAR)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Sydney, Australia</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;1533–1538</span> (<span class="ltx_text ltx_bib_language">en</span>).
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-72813-014-9</span>,
<a href="https://ieeexplore.ieee.org/document/8978036/" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1109/ICDAR.2019.00247" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. P. Majumder, N. Potti, S. Tata, J. B. Wendt, Q. Zhao, and M. Najork</span><span class="ltx_text ltx_bib_year"> (2020-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Representation Learning for Information Extraction from Form-like Documents</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Online</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;6495–6504</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://aclanthology.org/2020.acl-main.580" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.18653/v1/2020.acl-main.580" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p1" title="3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem ltx_bib_book">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. W. (. W. March and H. C. (. C. Wolff</span><span class="ltx_text ltx_bib_year"> (1917)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Calculus</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">New York : McGraw-Hill</span> (<span class="ltx_text ltx_bib_language">eng</span>).
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://archive.org/details/calculus00marciala" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A2.F1" title="Figure B.1 ‣ Appendix B Examples ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure B.1</span></a>.
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Moysset, C. Kermorvant, and C. Wolf</span><span class="ltx_text ltx_bib_year"> (2017-04)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Full-Page Text Recognition: Learning Where to Start and When to Stop</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:1704.08628 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1704.08628" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Papineni, S. Roukos, T. Ward, and W. Zhu</span><span class="ltx_text ltx_bib_year"> (2002-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Bleu: a Method for Automatic Evaluation of Machine Translation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Philadelphia, Pennsylvania, USA</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;311–318</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://aclanthology.org/P02-1040" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/1073083.1073135" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.p3" title="5.1 Metrics ‣ 5 Results &amp; Evaluation ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1</span></a>.
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Sebastian Spiegler</span><span class="ltx_text ltx_bib_year"> (2013-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Statistics of the Common Crawl Corpus 2012</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://docs.google.com/file/d/1_9698uglerxB9nAglvaHkEgU-iZNm1TvVGuCW7245-WGvZq47teNpb_uL5N9" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Shah, X. Chen, M. Rohrbach, and D. Parikh</span><span class="ltx_text ltx_bib_year"> (2019-02)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Cycle-Consistency for Robust Visual Question Answering</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:1902.05660 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1902.05660" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A2.F4" title="Figure B.4 ‣ Appendix B Examples ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure B.4</span></a>.
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P.Y. Simard, D. Steinkraus, and J.C. Platt</span><span class="ltx_text ltx_bib_year"> (2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Best practices for convolutional neural networks applied to visual document analysis</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">1</span>, <span class="ltx_text ltx_bib_place">Edinburgh, UK</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;958–963</span> (<span class="ltx_text ltx_bib_language">en</span>).
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-0-7695-1960-9</span>,
<a href="http://ieeexplore.ieee.org/document/1227801/" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1109/ICDAR.2003.1227801" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.p1" title="3.2 Data Augmentation ‣ 3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2</span></a>.
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. S. Singh</span><span class="ltx_text ltx_bib_year"> (2018-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Teaching Machines to Code: Neural Markup Generation with Visual Attention</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:1802.05415 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1802.05415" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Smith</span><span class="ltx_text ltx_bib_year"> (2007-09)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An Overview of the Tesseract OCR Engine</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Curitiba, Parana, Brazil</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;629–633</span> (<span class="ltx_text ltx_bib_language">en</span>).
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">ISSN: 1520-5363</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-0-7695-2822-9</span>,
<a href="http://ieeexplore.ieee.org/document/4376991/" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1109/ICDAR.2007.4376991" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Sorscher, R. Geirhos, S. Shekhar, S. Ganguli, and A. S. Morcos</span><span class="ltx_text ltx_bib_year"> (2022-11)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Beyond neural scaling laws: beating power law scaling via data pruning</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:2206.14486 [cs, stat]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/2206.14486" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.F5" title="Figure 5 ‣ 5 Results &amp; Evaluation ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>.
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia, A. Poulton, V. Kerkez, and R. Stojnic</span><span class="ltx_text ltx_bib_year"> (2022-11)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Galactica: A Large Language Model for Science</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:2211.09085 [cs, stat]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/2211.09085" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.48550/arXiv.2211.09085" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p4" title="3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin</span><span class="ltx_text ltx_bib_year"> (2017-12)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Attention Is All You Need</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:1706.03762 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1706.03762" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.48550/arXiv.1706.03762" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S3.p2" title="3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Wang and J. Liu</span><span class="ltx_text ltx_bib_year"> (2019-09)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Translating Math Formula Images to LaTeX Sequences Using Deep Neural Networks with Sequence-level Training</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:1908.11415 [cs, stat]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1908.11415" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.48550/arXiv.1908.11415" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Xu, Y. Xu, T. Lv, L. Cui, F. Wei, G. Wang, Y. Lu, D. Florencio, C. Zhang, W. Che, M. Zhang, and L. Zhou</span><span class="ltx_text ltx_bib_year"> (2022-01)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:2012.14740 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/2012.14740" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S3.p1" title="3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Xu, M. Li, L. Cui, S. Huang, F. Wei, and M. Zhou</span><span class="ltx_text ltx_bib_year"> (2020-08)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">LayoutLM: Pre-training of Text and Layout for Document Image Understanding</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;1192–1200</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:1912.13318 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1912.13318" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1145/3394486.3403172" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S3.p1" title="3 Model ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Yan, X. Zhang, L. Gao, K. Yuan, and Z. Tang</span><span class="ltx_text ltx_bib_year"> (2020-12)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">ConvMath: A Convolutional Sequence Network for Mathematical Expression Recognition</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:2012.12619 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/2012.12619" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Zhang, J. Du, and L. Dai</span><span class="ltx_text ltx_bib_year"> (2018-01)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Multi-Scale Attention with Dense Encoder for Handwritten Mathematical Expression Recognition</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:1801.03530 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1801.03530" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.48550/arXiv.1801.03530" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">W. Zhao, L. Gao, Z. Yan, S. Peng, L. Du, and Z. Zhang</span><span class="ltx_text ltx_bib_year"> (2021-05)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Handwritten Mathematical Expression Recognition with Bidirectionally Trained Transformer</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:2105.02412 [cs]</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/2105.02412" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<span class="ltx_ERROR undefined">\addappheadtotoc</span>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Dataset</h2>

<figure id="A1.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Name</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Number of Pages</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">arXiv</th>
<td class="ltx_td ltx_align_right ltx_border_t">7,511,745</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">PMC</th>
<td class="ltx_td ltx_align_right">536,319</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">IDL</th>
<td class="ltx_td ltx_align_right">446,777</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span class="ltx_text ltx_font_bold">Total</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span class="ltx_text ltx_font_bold">8,204,754</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table A.1</span>: </span><span class="ltx_text" style="font-size:90%;">Dataset composition</span></figcaption>
</figure>
<div id="A1.p1" class="ltx_para">
<p class="ltx_p">The most important data source is arXiv, making up <span id="A1.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=">91.5\%"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&gt;</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">91.5</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">%</span></span></span></span></span></span></span> of the corpus. On arXiv most research documents are paired with the LaTeX source code provided by the authors. The LaTeX source offers more information and is left unprocessed, unlike the XML format from PMC where equations and tables are frequently substituted with images. This allows us to select exactly which information we need to build the dataset.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Examples</h2>

<div id="A2.p1" class="ltx_para">
<p class="ltx_p">In this section we converted some pages from old text books using the Nougat base model. The text books from the <em class="ltx_emph ltx_font_italic">Internet Archive</em><span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a href="https://archive.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://archive.org/</a></span></span></span> and <em class="ltx_emph ltx_font_italic">Project Gutenberg</em><span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a href="https://www.gutenberg.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.gutenberg.org/</a></span></span></span> and are in public domain.

<br class="ltx_break">The performance for these scanned pages is noticeable worse than for digital-born documents. However, the model does generate sensible text for each page with few errors. For example see the first row of Fig. <a href="#A2.F1" title="Figure B.1 ‣ Appendix B Examples ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a>. Here the model mistakes the almost illegible exponent <span id="A2.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span> for <span id="A2.p1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\ast"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span></span></span></span></span></span>. In the second row of the same figure the model falls into a repetitive loop after predicting another comma instead of a dot. Similar problems can be seen in Fig. <a href="#A2.F2" title="Figure B.2 ‣ Appendix B Examples ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a>.

<br class="ltx_break">In Fig. <a href="#A2.F3" title="Figure B.3 ‣ Appendix B Examples ‣ Nougat: Neural Optical Understanding for Academic Documents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.3</span></a> we present pages, scanned with a mobile device, from a printed master thesis and the Nougat output. The model is robust to the artifacts that arise when hand-scanning a document.

<br class="ltx_break">Explore the examples in this section on the project page: <a href="https://facebookresearch.github.io/nougat" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://facebookresearch.github.io/nougat</a>.</p>
</div>
<figure id="A2.F1" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subfigure">
<figure id="A2.F1.fig1" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="x8.png" id="A2.F1.g1" class="ltx_graphics" width="242" height="355" alt=""></span></p>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A2.F1.fig2" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="x9.png" id="A2.F1.g2" class="ltx_graphics" width="313" height="443" alt=""></span></p>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A2.F1.fig3" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="x10.png" id="A2.F1.g3" class="ltx_graphics" width="237" height="355" alt=""></span></p>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A2.F1.fig4" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="x11.png" id="A2.F1.g4" class="ltx_graphics" width="313" height="443" alt=""></span></p>
</figure>
</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure B.1</span>: </span><span class="ltx_text" style="font-size:90%;">Example of an old calculus text book <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="Calculus" class="ltx_ref">32</a>]</cite>. </span></figcaption>
</figure>
<figure id="A2.F2" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subfigure">
<figure id="A2.F2.fig1" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="x12.png" id="A2.F2.g1" class="ltx_graphics" width="275" height="355" alt=""></span></p>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A2.F2.fig2" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="x13.png" id="A2.F2.g2" class="ltx_graphics" width="340" height="444" alt=""></span></p>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A2.F2.fig3" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="x14.png" id="A2.F2.g3" class="ltx_graphics" width="275" height="355" alt=""></span></p>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A2.F2.fig4" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="x15.png" id="A2.F2.g4" class="ltx_graphics" width="340" height="444" alt=""></span></p>
</figure>
</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure B.2</span>: </span><span class="ltx_text" style="font-size:90%;">A selection of pages from a NASA conference from 1970 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="Kinetics and Thermodynamics in High-Temperature Gases" class="ltx_ref">19</a>]</cite>.</span></figcaption>
</figure>
<figure id="A2.F3" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subfigure">
<figure id="A2.F3.fig1" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="x16.png" id="A2.F3.g1" class="ltx_graphics" width="274" height="355" alt=""></span></p>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A2.F3.fig2" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="x17.png" id="A2.F3.g2" class="ltx_graphics" width="313" height="443" alt=""></span></p>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A2.F3.fig3" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="x18.png" id="A2.F3.g3" class="ltx_graphics" width="274" height="355" alt=""></span></p>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A2.F3.fig4" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="x19.png" id="A2.F3.g4" class="ltx_graphics" width="313" height="443" alt=""></span></p>
</figure>
</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure B.3</span>: </span><span class="ltx_text" style="font-size:90%;">Scan of a modern thesis with a mobile device camera, with permission from the author. </span></figcaption>
</figure>
<figure id="A2.F4" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subfigure">
<figure id="A2.F4.fig1" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="x20.png" id="A2.F4.g1" class="ltx_graphics" width="274" height="355" alt=""></span></p>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A2.F4.fig2" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="x21.png" id="A2.F4.g2" class="ltx_graphics" width="313" height="443" alt=""></span></p>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A2.F4.fig3" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="x22.png" id="A2.F4.g3" class="ltx_graphics" width="251" height="355" alt=""></span></p>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A2.F4.fig4" class="ltx_figure ltx_align_center">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;"><img src="x23.png" id="A2.F4.g4" class="ltx_graphics" width="323" height="398" alt=""></span></p>
</figure>
</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure B.4</span>: </span><span class="ltx_text" style="font-size:90%;">Pages with tables. Upper: Fan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="Hierarchical Neural Story Generation" class="ltx_ref">14</a>]</cite> page 6, Lower: Shah et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref">36</a>]</cite> page 6</span></figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>


<script src="index.js" type="text/javascript"></script></body></html>