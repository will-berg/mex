@misc{liu_swin_2021,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	shorttitle = {Swin {Transformer}},
	url = {http://arxiv.org/abs/2103.14030},
	doi = {10.48550/arXiv.2103.14030},
	abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	month = aug,
	year = {2021},
	note = {arXiv:2103.14030 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\K5XE4PEY\\Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\MRY4D7JH\\2103.html:text/html},
}

@misc{kim_ocr-free_2022,
	title = {{OCR}-free {Document} {Understanding} {Transformer}},
	url = {http://arxiv.org/abs/2111.15664},
	doi = {10.48550/arXiv.2111.15664},
	abstract = {Understanding document images (e.g., invoices) is a core but challenging task since it requires complex functions such as reading text and a holistic understanding of the document. Current Visual Document Understanding (VDU) methods outsource the task of reading text to off-the-shelf Optical Character Recognition (OCR) engines and focus on the understanding task with the OCR outputs. Although such OCR-based approaches have shown promising performance, they suffer from 1) high computational costs for using OCR; 2) inflexibility of OCR models on languages or types of document; 3) OCR error propagation to the subsequent process. To address these issues, in this paper, we introduce a novel OCR-free VDU model named Donut, which stands for Document understanding transformer. As the first step in OCR-free VDU research, we propose a simple architecture (i.e., Transformer) with a pre-training objective (i.e., cross-entropy loss). Donut is conceptually simple yet effective. Through extensive experiments and analyses, we show a simple OCR-free VDU model, Donut, achieves state-of-the-art performances on various VDU tasks in terms of both speed and accuracy. In addition, we offer a synthetic data generator that helps the model pre-training to be flexible in various languages and domains. The code, trained model and synthetic data are available at https://github.com/clovaai/donut.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, Jeongyeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
	month = oct,
	year = {2022},
	note = {arXiv:2111.15664 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\46XEK5NC\\Kim et al. - 2022 - OCR-free Document Understanding Transformer.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\4H48GRTY\\2111.html:text/html},
}

@misc{deng_image--markup_2016,
	title = {Image-to-{Markup} {Generation} with {Coarse}-to-{Fine} {Attention}},
	url = {http://arxiv.org/abs/1609.04938},
	doi = {10.48550/arXiv.1609.04938},
	abstract = {We present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism. Our method is evaluated in the context of image-to-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup. We show that unlike neural OCR techniques using CTC-based models, attention-based approaches can tackle this non-standard OCR task. Our approach outperforms classical mathematical OCR systems by a large margin on in-domain rendered data, and, with pretraining, also performs well on out-of-domain handwritten data. To reduce the inference complexity associated with the attention-based approaches, we introduce a new coarse-to-fine attention layer that selects a support region before applying attention.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Deng, Yuntian and Kanervisto, Anssi and Ling, Jeffrey and Rush, Alexander M.},
	month = sep,
	year = {2016},
	note = {arXiv:1609.04938 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\X5QHJNRU\\Deng et al. - 2016 - Image-to-Markup Generation with Coarse-to-Fine Att.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\E7QSWZQ8\\1609.html:text/html},
}

@misc{wang_translating_2019,
	title = {Translating {Math} {Formula} {Images} to {LaTeX} {Sequences} {Using} {Deep} {Neural} {Networks} with {Sequence}-level {Training}},
	url = {http://arxiv.org/abs/1908.11415},
	doi = {10.48550/arXiv.1908.11415},
	abstract = {In this paper we propose a deep neural network model with an encoder-decoder architecture that translates images of math formulas into their LaTeX markup sequences. The encoder is a convolutional neural network (CNN) that transforms images into a group of feature maps. To better capture the spatial relationships of math symbols, the feature maps are augmented with 2D positional encoding before being unfolded into a vector. The decoder is a stacked bidirectional long short-term memory (LSTM) model integrated with the soft attention mechanism, which works as a language model to translate the encoder output into a sequence of LaTeX tokens. The neural network is trained in two steps. The first step is token-level training using the Maximum-Likelihood Estimation (MLE) as the objective function. At completion of the token-level training, the sequence-level training objective function is employed to optimize the overall model based on the policy gradient algorithm from reinforcement learning. Our design also overcomes the exposure bias problem by closing the feedback loop in the decoder during sequence-level training, i.e., feeding in the predicted token instead of the ground truth token at every time step. The model is trained and evaluated on the IM2LATEX-100K dataset and shows state-of-the-art performance on both sequence-based and image-based evaluation metrics.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Wang, Zelun and Liu, Jyh-Charn},
	month = sep,
	year = {2019},
	note = {arXiv:1908.11415 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\BAQZVWIB\\Wang and Liu - 2019 - Translating Math Formula Images to LaTeX Sequences.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\N27WC7IZ\\1908.html:text/html},
}

@misc{li_trocr_2022,
	title = {{TrOCR}: {Transformer}-based {Optical} {Character} {Recognition} with {Pre}-trained {Models}},
	shorttitle = {{TrOCR}},
	url = {http://arxiv.org/abs/2109.10282},
	doi = {10.48550/arXiv.2109.10282},
	abstract = {Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at {\textbackslash}url\{https://aka.ms/trocr\}.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Li, Minghao and Lv, Tengchao and Chen, Jingye and Cui, Lei and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Li, Zhoujun and Wei, Furu},
	month = sep,
	year = {2022},
	note = {arXiv:2109.10282 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\JRRYNQDN\\Li et al. - 2022 - TrOCR Transformer-based Optical Character Recogni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\LYFP6FUM\\2109.html:text/html},
}

@misc{appalaraju_docformer_2021,
	title = {{DocFormer}: {End}-to-{End} {Transformer} for {Document} {Understanding}},
	shorttitle = {{DocFormer}},
	url = {http://arxiv.org/abs/2106.11539},
	doi = {10.48550/arXiv.2106.11539},
	abstract = {We present DocFormer -- a multi-modal transformer based architecture for the task of Visual Document Understanding (VDU). VDU is a challenging problem which aims to understand documents in their varied formats (forms, receipts etc.) and layouts. In addition, DocFormer is pre-trained in an unsupervised fashion using carefully designed tasks which encourage multi-modal interaction. DocFormer uses text, vision and spatial features and combines them using a novel multi-modal self-attention layer. DocFormer also shares learned spatial embeddings across modalities which makes it easy for the model to correlate text to visual tokens and vice versa. DocFormer is evaluated on 4 different datasets each with strong baselines. DocFormer achieves state-of-the-art results on all of them, sometimes beating models 4x its size (in no. of parameters).},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Appalaraju, Srikar and Jasani, Bhavan and Kota, Bhargava Urala and Xie, Yusheng and Manmatha, R.},
	month = sep,
	year = {2021},
	note = {arXiv:2106.11539 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\KT55IE7C\\Appalaraju et al. - 2021 - DocFormer End-to-End Transformer for Document Und.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\CTJA8UII\\2106.html:text/html},
}

@misc{biten_ocr-idl_2022,
	title = {{OCR}-{IDL}: {OCR} {Annotations} for {Industry} {Document} {Library} {Dataset}},
	shorttitle = {{OCR}-{IDL}},
	url = {http://arxiv.org/abs/2202.12985},
	doi = {10.48550/arXiv.2202.12985},
	abstract = {Pretraining has proven successful in Document Intelligence tasks where deluge of documents are used to pretrain the models only later to be finetuned on downstream tasks. One of the problems of the pretraining approaches is the inconsistent usage of pretraining data with different OCR engines leading to incomparable results between models. In other words, it is not obvious whether the performance gain is coming from diverse usage of amount of data and distinct OCR engines or from the proposed models. To remedy the problem, we make public the OCR annotations for IDL documents using commercial OCR engine given their superior performance over open source OCR models. The contributed dataset (OCR-IDL) has an estimated monetary value over 20K US\$. It is our hope that OCR-IDL can be a starting point for future works on Document Intelligence. All of our data and its collection process with the annotations can be found in https://github.com/furkanbiten/idl\_data.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Biten, Ali Furkan and Tito, Rubèn and Gomez, Lluis and Valveny, Ernest and Karatzas, Dimosthenis},
	month = feb,
	year = {2022},
	note = {arXiv:2202.12985 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\47HIVKD5\\Biten et al. - 2022 - OCR-IDL OCR Annotations for Industry Document Lib.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\ARXK8IBM\\2202.html:text/html},
}

@misc{taylor_galactica_2022,
	title = {Galactica: {A} {Large} {Language} {Model} for {Science}},
	shorttitle = {Galactica},
	url = {http://arxiv.org/abs/2211.09085},
	doi = {10.48550/arXiv.2211.09085},
	abstract = {Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2\% versus 49.0\%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3\% to 35.7\%, and PaLM 540B on MATH with a score of 20.4\% versus 8.8\%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6\% and 52.9\%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
	month = nov,
	year = {2022},
	note = {arXiv:2211.09085 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\TMWWABV3\\Taylor et al. - 2022 - Galactica A Large Language Model for Science.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\LB59FKT4\\2211.html:text/html},
}


@inproceedings{xu_layoutlm_2020,
	title = {{LayoutLM}: {Pre}-training of {Text} and {Layout} for {Document} {Image} {Understanding}},
	shorttitle = {{LayoutLM}},
	url = {http://arxiv.org/abs/1912.13318},
	doi = {10.1145/3394486.3403172},
	abstract = {Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the {\textbackslash}textbf\{LayoutLM\} to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at {\textbackslash}url\{https://aka.ms/layoutlm\}.},
	urldate = {2023-02-07},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	author = {Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
	month = aug,
	year = {2020},
	note = {arXiv:1912.13318 [cs]},
	keywords = {Computer Science - Computation and Language},
	pages = {1192--1200},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\53FCRQI9\\Xu et al. - 2020 - LayoutLM Pre-training of Text and Layout for Docu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\F67ICH8T\\1912.html:text/html},
}

@misc{xu_layoutlmv2_2022,
	title = {{LayoutLMv2}: {Multi}-modal {Pre}-training for {Visually}-{Rich} {Document} {Understanding}},
	shorttitle = {{LayoutLMv2}},
	url = {http://arxiv.org/abs/2012.14740},
	abstract = {Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 \${\textbackslash}to\$ 0.8420), CORD (0.9493 \${\textbackslash}to\$ 0.9601), SROIE (0.9524 \${\textbackslash}to\$ 0.9781), Kleister-NDA (0.8340 \${\textbackslash}to\$ 0.8520), RVL-CDIP (0.9443 \${\textbackslash}to\$ 0.9564), and DocVQA (0.7295 \${\textbackslash}to\$ 0.8672). We made our model and code publicly available at {\textbackslash}url\{https://aka.ms/layoutlmv2\}.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Xu, Yang and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wei, Furu and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Che, Wanxiang and Zhang, Min and Zhou, Lidong},
	month = jan,
	year = {2022},
	note = {arXiv:2012.14740 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\WLB4UYHB\\Xu et al. - 2022 - LayoutLMv2 Multi-modal Pre-training for Visually-.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\FN3ZQ9CQ\\2012.html:text/html},
}

@misc{huang_layoutlmv3_2022,
	title = {{LayoutLMv3}: {Pre}-training for {Document} {AI} with {Unified} {Text} and {Image} {Masking}},
	shorttitle = {{LayoutLMv3}},
	url = {http://arxiv.org/abs/2204.08387},
	abstract = {Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose {\textbackslash}textbf\{LayoutLMv3\} to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. The code and models are publicly available at {\textbackslash}url\{https://aka.ms/layoutlmv3\}.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Huang, Yupan and Lv, Tengchao and Cui, Lei and Lu, Yutong and Wei, Furu},
	month = jul,
	year = {2022},
	note = {arXiv:2204.08387 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\YB2C4LSD\\Huang et al. - 2022 - LayoutLMv3 Pre-training for Document AI with Unif.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\XEANAZTT\\2204.html:text/html},
}

@inproceedings{majumder_representation_2020,
	address = {Online},
	title = {Representation {Learning} for {Information} {Extraction} from {Form}-like {Documents}},
	url = {https://aclanthology.org/2020.acl-main.580},
	doi = {10.18653/v1/2020.acl-main.580},
	abstract = {We propose a novel approach using representation learning for tackling the problem of extracting structured information from form-like document images. We propose an extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document. These learned representations are not only useful in solving the extraction task for unseen document templates from two different domains but are also interpretable, as we show using loss cases.},
	urldate = {2023-02-07},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Majumder, Bodhisattwa Prasad and Potti, Navneet and Tata, Sandeep and Wendt, James Bradley and Zhao, Qi and Najork, Marc},
	month = jul,
	year = {2020},
	pages = {6495--6504},
	file = {Full Text PDF:C\:\\Users\\lblecher\\Zotero\\storage\\XTCCFAK5\\Majumder et al. - 2020 - Representation Learning for Information Extraction.pdf:application/pdf},
}


@misc{lewis_bart_2019,
	title = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
	shorttitle = {{BART}},
	url = {http://arxiv.org/abs/1910.13461},
	doi = {10.48550/arXiv.1910.13461},
	abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	month = oct,
	year = {2019},
	note = {arXiv:1910.13461 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\YFB9G3TF\\Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\25RSKRIT\\1910.html:text/html},
}

@misc{lu_wang_online_2013,
	title = {Online publishing via {pdf2htmlEX}},
	url = {https://www.tug.org/TUGboat/tb34-3/tb108wang.pdf},
	abstract = {The Web has long become an essential part of our
lives. While web technologies have been actively
developed for years, there is still a large gap between
web and traditional paper publishing. For example,
the PDF format, the de facto standard for publishing,
is not supported in the HTML standard; and the
most powerful typesetting system, TEX, cannot be
integrated perfectly.
Despite of the long history of people trying to
convert TEX or PDF into HTML, some are focused on
only a small fraction of features, e.g. text, formulas
or images; some are too old to support new features
in the HTML standard such as font embedding or
linear transformations (e.g. rotation); some display
everything in images at the cost of larger sizes.
In this article, while we survey and compare
existing methods of publishing TEX or PDF documents online, a new approach is attempted to attack
this issue. We introduce an open source program,
called pdf2htmlEX, which is a general PDF to HTML
converter and publishing tool with high fidelity. It
presents PDF elements with corresponding native
HTML elements, in order to achieve high accuracy
and small size. The flexible design also makes it
useful for a variety of use cases in online publishing.
Obviously TEX users can immediately benefit with
zero learning cost, just like dvipdf while people were
still using DVI. More information is available at the
home page:
https://github.com/coolwanglu/pdf2htmlex},
	urldate = {2023-02-09},
	publisher = {TUGboat},
	author = {{Lu Wang} and {Wanmin Liu}},
	year = {2013},
	file = {tb108wang.pdf:C\:\\Users\\lblecher\\Zotero\\storage\\H2ZCFBYG\\tb108wang.pdf:application/pdf},
}

@misc{blecher_pix2tex_2023,
	title = {pix2tex - {LaTeX} {OCR}},
	copyright = {MIT},
	url = {https://github.com/lukas-blecher/LaTeX-OCR},
	abstract = {pix2tex: Using a ViT to convert images of equations into LaTeX code.},
	urldate = {2023-02-09},
	author = {Blecher, Lukas},
	month = feb,
	year = {2023},
	note = {original-date: 2020-12-11T16:35:13Z},
	keywords = {dataset, deep-learning, im2latex, im2markup, im2text, image-processing, image2text, latex, latex-ocr, machine-learning, math-ocr, ocr, python, pytorch, transformer, vision-transformer, vit},
}

@misc{singh_teaching_2018,
	title = {Teaching {Machines} to {Code}: {Neural} {Markup} {Generation} with {Visual} {Attention}},
	shorttitle = {Teaching {Machines} to {Code}},
	url = {http://arxiv.org/abs/1802.05415},
	abstract = {We present a neural transducer model with visual attention that learns to generate LaTeX markup of a real-world math formula given its image. Applying sequence modeling and transduction techniques that have been very successful across modalities such as natural language, image, handwriting, speech and audio; we construct an image-to-markup model that learns to produce syntactically and semantically correct LaTeX markup code over 150 words long and achieves a BLEU score of 89\%; improving upon the previous state-of-art for the Im2Latex problem. We also demonstrate with heat-map visualization how attention helps in interpreting the model and can pinpoint (detect and localize) symbols on the image accurately despite having been trained without any bounding box data.},
	urldate = {2023-02-09},
	publisher = {arXiv},
	author = {Singh, Sumeet S.},
	month = jun,
	year = {2018},
	note = {arXiv:1802.05415 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\CKAP7S5A\\Singh - 2018 - Teaching Machines to Code Neural Markup Generatio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\UVGKQ83P\\1802.html:text/html},
}

@misc{zhao_handwritten_2021,
	title = {Handwritten {Mathematical} {Expression} {Recognition} with {Bidirectionally} {Trained} {Transformer}},
	url = {http://arxiv.org/abs/2105.02412},
	abstract = {Encoder-decoder models have made great progress on handwritten mathematical expression recognition recently. However, it is still a challenge for existing methods to assign attention to image features accurately. Moreover, those encoder-decoder models usually adopt RNN-based models in their decoder part, which makes them inefficient in processing long \${\textbackslash}LaTeX\{\}\$ sequences. In this paper, a transformer-based decoder is employed to replace RNN-based ones, which makes the whole model architecture very concise. Furthermore, a novel training strategy is introduced to fully exploit the potential of the transformer in bidirectional language modeling. Compared to several methods that do not use data augmentation, experiments demonstrate that our model improves the ExpRate of current state-of-the-art methods on CROHME 2014 by 2.23\%. Similarly, on CROHME 2016 and CROHME 2019, we improve the ExpRate by 1.92\% and 2.28\% respectively.},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Zhao, Wenqi and Gao, Liangcai and Yan, Zuoyu and Peng, Shuai and Du, Lin and Zhang, Ziyin},
	month = may,
	year = {2021},
	note = {arXiv:2105.02412 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\AQPZK7EW\\2105.html:text/html;Full Text:C\:\\Users\\lblecher\\Zotero\\storage\\P6MHL2FK\\Zhao et al. - 2021 - Handwritten Mathematical Expression Recognition wi.pdf:application/pdf},
}


@misc{yan_convmath_2020,
	title = {{ConvMath}: {A} {Convolutional} {Sequence} {Network} for {Mathematical} {Expression} {Recognition}},
	shorttitle = {{ConvMath}},
	url = {http://arxiv.org/abs/2012.12619},
	abstract = {Despite the recent advances in optical character recognition (OCR), mathematical expressions still face a great challenge to recognize due to their two-dimensional graphical layout. In this paper, we propose a convolutional sequence modeling network, ConvMath, which converts the mathematical expression description in an image into a LaTeX sequence in an end-to-end way. The network combines an image encoder for feature extraction and a convolutional decoder for sequence generation. Compared with other Long Short Term Memory(LSTM) based encoder-decoder models, ConvMath is entirely based on convolution, thus it is easy to perform parallel computation. Besides, the network adopts multi-layer attention mechanism in the decoder, which allows the model to align output symbols with source feature vectors automatically, and alleviates the problem of lacking coverage while training the model. The performance of ConvMath is evaluated on an open dataset named IM2LATEX-100K, including 103556 samples. The experimental results demonstrate that the proposed network achieves state-of-the-art accuracy and much better efficiency than previous methods.},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Yan, Zuoyu and Zhang, Xiaode and Gao, Liangcai and Yuan, Ke and Tang, Zhi},
	month = dec,
	year = {2020},
	note = {arXiv:2012.12619 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\TT5A64IZ\\Yan et al. - 2020 - ConvMath A Convolutional Sequence Network for Mat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\3HTW3G5N\\2012.html:text/html},
}

@article{awal_global_2014,
	title = {A global learning approach for an online handwritten mathematical expression recognition system},
	volume = {35},
	issn = {0167-8655},
	abstract = {Highlights 2D languages recognition (ex: mathematical expressions) is a challenging problem. In this work, classic steps of 2D language recognition are applied simultaneously. A Global approach allows learning symbols and a reject class directly from expressions. Structural models are directly learnt from mathematical expressions. A new public expression database is proposed and used to evaluate the system. Despite the recent advances in handwriting recognition, handwritten two-dimensional (2D) languages are still a challenge. Electrical schemas, chemical equations and mathematical expressions (MEs) are examples of such 2D languages. In this case, the recognition problem is particularly difficult due to the two dimensional layout of the language. This paper presents an online handwritten mathematical expression recognition system that handles mathematical expression recognition as a simultaneous optimization of expression segmentation, symbol recognition, and 2D structure recognition under the restriction of a mathematical expression grammar. The originality of the approach is a global strategy allowing learning mathematical symbols and spatial relations directly from complete expressions. A new contextual modeling is proposed for combining syntactic and structural information. Those models are used to find the most likely combination of segmentation/recognition hypotheses proposed by a 2D segmentation scheme. Thus, models are based on structural information concerning the symbol layout. The system is tested with a new public database of mathematical expressions which was used in the CHROME competition. We have also produced a large base of semi-synthetic expressions which are used to train and test the global learning approach. We obtain very promising results on both synthetic and real expressions databases, as well as in the recent CHROME competition.},
	number = {C},
	journal = {Pattern Recognition Letters},
	author = {Awal, Ahmad-Montaser and Mouchre, Harold and Viard-Gaudin, Christian},
	month = jan,
	year = {2014},
	keywords = {Bidimensional languages, Handwriting recognition, Math recognition, Structural pattern recognition, Syntactic pattern recognition},
	pages = {68--77},
}

@misc{zhang_multi-scale_2018,
	title = {Multi-{Scale} {Attention} with {Dense} {Encoder} for {Handwritten} {Mathematical} {Expression} {Recognition}},
	url = {http://arxiv.org/abs/1801.03530},
	doi = {10.48550/arXiv.1801.03530},
	abstract = {Handwritten mathematical expression recognition is a challenging problem due to the complicated two-dimensional structures, ambiguous handwriting input and variant scales of handwritten math symbols. To settle this problem, we utilize the attention based encoder-decoder model that recognizes mathematical expression images from two-dimensional layouts to one-dimensional LaTeX strings. We improve the encoder by employing densely connected convolutional networks as they can strengthen feature extraction and facilitate gradient propagation especially on a small training set. We also present a novel multi-scale attention model which is employed to deal with the recognition of math symbols in different scales and save the fine-grained details that will be dropped by pooling operations. Validated on the CROHME competition task, the proposed method significantly outperforms the state-of-the-art methods with an expression recognition accuracy of 52.8\% on CROHME 2014 and 50.1\% on CROHME 2016, by only using the official training dataset.},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Zhang, Jianshu and Du, Jun and Dai, Lirong},
	month = jan,
	year = {2018},
	note = {arXiv:1801.03530 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\4F8VNQKZ\\Zhang et al. - 2018 - Multi-Scale Attention with Dense Encoder for Handw.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\IH3D6E4M\\1801.html:text/html},
}

@inproceedings{he_context-aware_2016,
	title = {Context-aware mathematical expression recognition: {An} end-to-end framework and a benchmark},
	shorttitle = {Context-aware mathematical expression recognition},
	doi = {10.1109/ICPR.2016.7900135},
	abstract = {In this paper we propose a novel end-to-end framework for mathematical expression (ME) recognition. The method uses a convolutional neural network (CNN) to perform mathematical symbol detection and recognition simultaneously incorporating spatial context, and can handle multi-part and touching symbols effectively. To evaluate the performance, we provide a benchmark that contains MEs both from real-life and synthetic data. Images in our dataset undergo multiple variations such as viewpoint, illumination and background. For training, we use pure synthetic data for saving human labeling effort. The proposed method achieved 87\% accuracy of total correct for clear images and 45\% for cluttered ones.},
	booktitle = {2016 23rd {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {He, Wenhao and Luo, Yuxuan and Yin, Fei and Hu, Han and Han, Junyu and Ding, Errui and Liu, Cheng-Lin},
	month = dec,
	year = {2016},
	keywords = {Benchmark testing, Context, Hidden Markov models, Image segmentation, Pattern recognition, Pipelines, Training},
	pages = {3246--3251},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lblecher\\Zotero\\storage\\2TYHTWRD\\7900135.html:text/html},
}

@inproceedings{le_training_2017,
	title = {Training an {End}-to-{End} {System} for {Handwritten} {Mathematical} {Expression} {Recognition} by {Generated} {Patterns}},
	volume = {01},
	doi = {10.1109/ICDAR.2017.175},
	abstract = {Motivated by recent successes in neural machine translation and image caption generation, we present an end-to-end system to recognize Online Handwritten Mathematical Expressions (OHMEs). Our system has three parts: a convolution neural network for feature extraction, a bidirectional LSTM for encoding extracted features, and an LSTM and an attention model for generating target LaTex. For recognizing complex structures, our system needs large data for training. We propose local and global distortion models for generating OHMEs from the CROHME database. We evaluate the end-to-end system on the CROHME database and the generated databases. The experiential results show that the end-to-end system achieves 28.09\% and 35.19\% recognition rates on CROHME without and with the generated data, respectively.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Le, Anh Duc and Nakagawa, Masaki},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Handwriting recognition, Hidden Markov models, Training, Databases, Distortion, Encoder-Decoder Model, End-to-End Model, Feature extraction, Mathematical model, Online Handwritten Mathematical Expression Recognition, Patterns Generation},
	pages = {1056--1061},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lblecher\\Zotero\\storage\\476DWMPK\\8270106.html:text/html},
}

@article{alvaro_recognition_2014,
	series = {Frontiers in {Handwriting} {Processing}},
	title = {Recognition of on-line handwritten mathematical expressions using {2D} stochastic context-free grammars and hidden {Markov} models},
	volume = {35},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S016786551200308X},
	doi = {10.1016/j.patrec.2012.09.023},
	abstract = {This paper describes a formal model for the recognition of on-line handwritten mathematical expressions using 2D stochastic context-free grammars and hidden Markov models. Hidden Markov models are used to recognize mathematical symbols, and a stochastic context-free grammar is used to model the relation between these symbols. This formal model makes possible to use classic algorithms for parsing and stochastic estimation. In this way, first, the model is able to capture many of variability phenomena that appear in on-line handwritten mathematical expressions during the training process. And second, the parsing process can make decisions taking into account only stochastic information, and avoiding heuristic decisions. The proposed model participated in a contest of mathematical expression recognition and it obtained the best results at different levels.},
	language = {en},
	urldate = {2023-02-13},
	journal = {Pattern Recognition Letters},
	author = {Álvaro, Francisco and Sánchez, Joan-Andreu and Benedí, José-Miguel},
	month = jan,
	year = {2014},
	keywords = {Handwriting recognition, Mathematical expression recognition, Spatial relations, Stochastic parsing, Structural analysis, Symbol recognition},
	pages = {58--67},
	file = {ScienceDirect Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\PWALLD4H\\S016786551200308X.html:text/html},
}

@misc{lopez_grobid_2023,
	title = {{GROBID}},
	copyright = {Apache-2.0},
	url = {https://github.com/kermitt2/grobid},
	abstract = {A machine learning software for extracting information from scholarly documents},
	urldate = {2023-02-13},
	author = {Lopez, Patrice},
	month = feb,
	year = {2023},
	note = {original-date: 2012-09-13T15:48:54Z},
	keywords = {deep-learning, machine-learning, bibliographical-references, crf, fulltext, hamburger-to-cow, metadata, pdf, scientific-articles},
}


@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2023-02-14},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\TWQQCJDE\\2010.html:text/html},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2023-02-14},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\EYBBW2SS\\Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\9EECLHMP\\1706.html:text/html},
}

@misc{diaz_rethinking_2021,
	title = {Rethinking {Text} {Line} {Recognition} {Models}},
	url = {http://arxiv.org/abs/2104.07787},
	abstract = {In this paper, we study the problem of text line recognition. Unlike most approaches targeting specific domains such as scene-text or handwritten documents, we investigate the general problem of developing a universal architecture that can extract text from any image, regardless of source or input modality. We consider two decoder families (Connectionist Temporal Classification and Transformer) and three encoder modules (Bidirectional LSTMs, Self-Attention, and GRCLs), and conduct extensive experiments to compare their accuracy and performance on widely used public datasets of scene and handwritten text. We find that a combination that so far has received little attention in the literature, namely a Self-Attention encoder coupled with the CTC decoder, when compounded with an external language model and trained on both public and internal data, outperforms all the others in accuracy and computational complexity. Unlike the more common Transformer-based models, this architecture can handle inputs of arbitrary length, a requirement for universal line recognition. Using an internal dataset collected from multiple sources, we also expose the limitations of current public datasets in evaluating the accuracy of line recognizers, as the relatively narrow image width and sequence length distributions do not allow to observe the quality degradation of the Transformer approach when applied to the transcription of long lines.},
	urldate = {2023-02-14},
	publisher = {arXiv},
	author = {Diaz, Daniel Hernandez and Qin, Siyang and Ingle, Reeve and Fujii, Yasuhisa and Bissacco, Alessandro},
	month = apr,
	year = {2021},
	note = {arXiv:2104.07787 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\62W6VYKZ\\Diaz et al. - 2021 - Rethinking Text Line Recognition Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\78T3QTJ6\\2104.html:text/html},
}

@misc{bautista_scene_2022,
	title = {Scene {Text} {Recognition} with {Permuted} {Autoregressive} {Sequence} {Models}},
	url = {http://arxiv.org/abs/2207.06966},
	abstract = {Context-aware STR methods typically use internal autoregressive (AR) language models (LM). Inherent limitations of AR models motivated two-stage methods which employ an external LM. The conditional independence of the external LM on the input image may cause it to erroneously rectify correct predictions, leading to significant inefficiencies. Our method, PARSeq, learns an ensemble of internal AR LMs with shared weights using Permutation Language Modeling. It unifies context-free non-AR and context-aware AR inference, and iterative refinement using bidirectional context. Using synthetic training data, PARSeq achieves state-of-the-art (SOTA) results in STR benchmarks (91.9\% accuracy) and more challenging datasets. It establishes new SOTA results (96.0\% accuracy) when trained on real data. PARSeq is optimal on accuracy vs parameter count, FLOPS, and latency because of its simple, unified structure and parallel token processing. Due to its extensive use of attention, it is robust on arbitrarily-oriented text which is common in real-world images. Code, pretrained weights, and data are available at: https://github.com/baudm/parseq.},
	urldate = {2023-02-14},
	publisher = {arXiv},
	author = {Bautista, Darwin and Atienza, Rowel},
	month = jul,
	year = {2022},
	note = {arXiv:2207.06966 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\TLIRP779\\Bautista and Atienza - 2022 - Scene Text Recognition with Permuted Autoregressiv.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\4QP955X9\\2207.html:text/html},
}

@misc{moysset_full-page_2017,
	title = {Full-{Page} {Text} {Recognition}: {Learning} {Where} to {Start} and {When} to {Stop}},
	shorttitle = {Full-{Page} {Text} {Recognition}},
	url = {http://arxiv.org/abs/1704.08628},
	abstract = {Text line detection and localization is a crucial step for full page document analysis, but still suffers from heterogeneity of real life documents. In this paper, we present a new approach for full page text recognition. Localization of the text lines is based on regressions with Fully Convolutional Neural Networks and Multidimensional Long Short-Term Memory as contextual layers. In order to increase the efficiency of this localization method, only the position of the left side of the text lines are predicted. The text recognizer is then in charge of predicting the end of the text to recognize. This method has shown good results for full page text recognition on the highly heterogeneous Maurdor dataset.},
	urldate = {2023-02-15},
	publisher = {arXiv},
	author = {Moysset, Bastien and Kermorvant, Christopher and Wolf, Christian},
	month = apr,
	year = {2017},
	note = {arXiv:1704.08628 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\BWGDV22L\\Moysset et al. - 2017 - Full-Page Text Recognition Learning Where to Star.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\87D92JR2\\1704.html:text/html},
}


@misc{davis_end--end_2022,
	title = {End-to-end {Document} {Recognition} and {Understanding} with {Dessurt}},
	url = {http://arxiv.org/abs/2203.16618},
	abstract = {We introduce Dessurt, a relatively simple document understanding transformer capable of being fine-tuned on a greater variety of document tasks than prior methods. It receives a document image and task string as input and generates arbitrary text autoregressively as output. Because Dessurt is an end-to-end architecture that performs text recognition in addition to the document understanding, it does not require an external recognition model as prior methods do. Dessurt is a more flexible model than prior methods and is able to handle a variety of document domains and tasks. We show that this model is effective at 9 different dataset-task combinations.},
	urldate = {2023-02-15},
	publisher = {arXiv},
	author = {Davis, Brian and Morse, Bryan and Price, Bryan and Tensmeyer, Chris and Wigington, Curtis and Morariu, Vlad},
	month = jun,
	year = {2022},
	note = {arXiv:2203.16618 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\WY9XAGP3\\Davis et al. - 2022 - End-to-end Document Recognition and Understanding .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\IDZ54NKF\\2203.html:text/html},
}

@inproceedings{clark_pdffigures_2016,
	address = {Newark New Jersey USA},
	title = {{PDFFigures} 2.0: {Mining} {Figures} from {Research} {Papers}},
	isbn = {978-1-4503-4229-2},
	shorttitle = {{PDFFigures} 2.0},
	url = {https://dl.acm.org/doi/10.1145/2910896.2910904},
	doi = {10.1145/2910896.2910904},
	abstract = {Figures and tables are key sources of information in many scholarly documents. However, current academic search engines do not make use of ﬁgures and tables when semantically parsing documents or presenting document summaries to users. To facilitate these applications we develop an algorithm that extracts ﬁgures, tables, and captions from documents called “PDFFigures 2.0.” Our proposed approach analyzes the structure of individual pages by detecting captions, graphical elements, and chunks of body text, and then locates ﬁgures and tables by reasoning about the empty regions within that text. To evaluate our work, we introduce a new dataset of computer science papers, along with ground truth labels for the locations of the ﬁgures, tables, and captions within them. Our algorithm achieves impressive results (94\% precision at 90\% recall) on this dataset surpassing previous state of the art. Further, we show how our framework was used to extract ﬁgures from a corpus of over one million papers, and how the resulting extractions were integrated into the user interface of a smart academic search engine, Semantic Scholar (www.semanticscholar.org). Finally, we present results of exploratory data analysis completed on the extracted ﬁgures as well as an extension of our method for the task of section title extraction. We release our dataset and code on our project webpage for enabling future research (http://pdﬃgures2.allenai.org).},
	language = {en},
	urldate = {2023-02-15},
	booktitle = {Proceedings of the 16th {ACM}/{IEEE}-{CS} on {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {ACM},
	author = {Clark, Christopher and Divvala, Santosh},
	month = jun,
	year = {2016},
	pages = {143--152},
	file = {Clark and Divvala - 2016 - PDFFigures 2.0 Mining Figures from Research Paper.pdf:C\:\\Users\\lblecher\\Zotero\\storage\\FCBFLWB2\\Clark and Divvala - 2016 - PDFFigures 2.0 Mining Figures from Research Paper.pdf:application/pdf},
}

@article{harris_distributional_1954,
	title = {Distributional {Structure}},
	volume = {10},
	url = {https://doi.org/10.1080/00437956.1954.11659520},
	doi = {10.1080/00437956.1954.11659520},
	number = {2-3},
	journal = {\textit{WORD}},
	author = {Harris, Zellig S.},
	year = {1954},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/00437956.1954.11659520},
	pages = {146--162},
}

@inproceedings{smith_overview_2007,
	address = {Curitiba, Parana, Brazil},
	title = {An {Overview} of the {Tesseract} {OCR} {Engine}},
	isbn = {978-0-7695-2822-9},
	url = {http://ieeexplore.ieee.org/document/4376991/},
	doi = {10.1109/ICDAR.2007.4376991},
	abstract = {The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy[1], is described in a comprehensive overview. Emphasis is placed on aspects that are novel or at least unusual in an OCR engine, including in particular the line finding, features/classification methods, and the adaptive classifier.},
	language = {en},
	urldate = {2023-02-15},
	booktitle = {Ninth {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR} 2007) {Vol} 2},
	publisher = {IEEE},
	author = {Smith, R.},
	month = sep,
	year = {2007},
	note = {ISSN: 1520-5363},
	pages = {629--633},
	file = {Smith - 2007 - An Overview of the Tesseract OCR Engine.pdf:C\:\\Users\\lblecher\\Zotero\\storage\\85R4RV5E\\Smith - 2007 - An Overview of the Tesseract OCR Engine.pdf:application/pdf},
}

@article{levenshtein_binary_1965,
	title = {Binary codes capable of correcting deletions, insertions, and reversals},
	url = {https://www.semanticscholar.org/paper/Binary-codes-capable-of-correcting-deletions%2C-and-Levenshtein/b2f8876482c97e804bb50a5e2433881ae31d0cdd},
	abstract = {Semantic Scholar extracted view of "Binary codes capable of correcting deletions, insertions, and reversals" by V. Levenshtein},
	urldate = {2023-02-15},
	journal = {Soviet physics. Doklady},
	author = {Levenshtein, V.},
	year = {1965},
}

@misc{holtzman_curious_2020,
	title = {The {Curious} {Case} of {Neural} {Text} {Degeneration}},
	url = {http://arxiv.org/abs/1904.09751},
	abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
	urldate = {2023-02-15},
	publisher = {arXiv},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	month = feb,
	year = {2020},
	note = {arXiv:1904.09751 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\AQFSJZ9V\\Holtzman et al. - 2020 - The Curious Case of Neural Text Degeneration.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\VGK6NTFJ\\1904.html:text/html},
}

@inproceedings{simard_best_2003,
	address = {Edinburgh, UK},
	title = {Best practices for convolutional neural networks applied to visual document analysis},
	volume = {1},
	isbn = {978-0-7695-1960-9},
	url = {http://ieeexplore.ieee.org/document/1227801/},
	doi = {10.1109/ICDAR.2003.1227801},
	abstract = {Neural networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural networks. The most important practice is getting a training set as large as possible: we expand the training set by adding a new form of distorted data. The next most important practice is that convolutional neural networks are better suited for visual document tasks than fully connected networks. We propose that a simple “do-it-yourself” implementation of convolution with a flexible architecture is suitable for many visual document problems. This simple convolutional neural network does not require complex methods, such as momentum, weight decay, structuredependent learning rates, averaging layers, tangent prop, or even finely-tuning the architecture. The end result is a very simple yet general architecture which can yield state-of-the-art performance for document analysis. We illustrate our claims on the MNIST set of English digit images.},
	language = {en},
	urldate = {2023-02-15},
	booktitle = {Seventh {International} {Conference} on {Document} {Analysis} and {Recognition}, 2003. {Proceedings}.},
	publisher = {IEEE Comput. Soc},
	author = {Simard, P.Y. and Steinkraus, D. and Platt, J.C.},
	year = {2003},
	pages = {958--963},
	file = {Simard et al. - 2003 - Best practices for convolutional neural networks a.pdf:C\:\\Users\\lblecher\\Zotero\\storage\\LB5UVCX7\\Simard et al. - 2003 - Best practices for convolutional neural networks a.pdf:application/pdf},
}

@inproceedings{papineni_bleu_2002,
	address = {Philadelphia, Pennsylvania, USA},
	title = {Bleu: a {Method} for {Automatic} {Evaluation} of {Machine} {Translation}},
	shorttitle = {Bleu},
	url = {https://aclanthology.org/P02-1040},
	doi = {10.3115/1073083.1073135},
	urldate = {2023-02-16},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	month = jul,
	year = {2002},
	pages = {311--318},
	file = {Full Text PDF:C\:\\Users\\lblecher\\Zotero\\storage\\8YCNKSV3\\Papineni et al. - 2002 - Bleu a Method for Automatic Evaluation of Machine.pdf:application/pdf},
}

@inproceedings{banerjee_meteor_2005,
	address = {Ann Arbor, Michigan},
	title = {{METEOR}: {An} {Automatic} {Metric} for {MT} {Evaluation} with {Improved} {Correlation} with {Human} {Judgments}},
	shorttitle = {{METEOR}},
	url = {https://aclanthology.org/W05-0909},
	urldate = {2023-02-16},
	booktitle = {Proceedings of the {ACL} {Workshop} on {Intrinsic} and {Extrinsic} {Evaluation} {Measures} for {Machine} {Translation} and/or {Summarization}},
	publisher = {Association for Computational Linguistics},
	author = {Banerjee, Satanjeev and Lavie, Alon},
	month = jun,
	year = {2005},
	pages = {65--72},
	file = {Full Text PDF:C\:\\Users\\lblecher\\Zotero\\storage\\T8FGVNMU\\Banerjee and Lavie - 2005 - METEOR An Automatic Metric for MT Evaluation with.pdf:application/pdf},
}

@misc{sebastian_spiegler_statistics_2013,
	title = {Statistics of the {Common} {Crawl} {Corpus} 2012},
	url = {https://docs.google.com/file/d/1_9698uglerxB9nAglvaHkEgU-iZNm1TvVGuCW7245-WGvZq47teNpb_uL5N9},
	urldate = {2023-02-22},
	author = {{Sebastian Spiegler}},
	month = jun,
	year = {2013},
	file = {Sebastian Spiegler - 2013 - Statistics of the Common Crawl Corpus 2012.pdf:C\:\\Users\\lblecher\\Zotero\\storage\\DZI9CDZA\\Sebastian Spiegler - 2013 - Statistics of the Common Crawl Corpus 2012.pdf:application/pdf},
}

@inproceedings{lo_s2orc_2020,
	address = {Online},
	title = {{S2ORC}: {The} {Semantic} {Scholar} {Open} {Research} {Corpus}},
	shorttitle = {{S2ORC}},
	url = {https://aclanthology.org/2020.acl-main.447},
	doi = {10.18653/v1/2020.acl-main.447},
	abstract = {We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text.},
	urldate = {2023-02-28},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Lo, Kyle and Wang, Lucy Lu and Neumann, Mark and Kinney, Rodney and Weld, Daniel},
	month = jul,
	year = {2020},
	pages = {4969--4983},
	file = {Full Text PDF:C\:\\Users\\lblecher\\Zotero\\storage\\8QUHVMDC\\Lo et al. - 2020 - S2ORC The Semantic Scholar Open Research Corpus.pdf:application/pdf},
}

@article{buslaev_albumentations_2020,
	title = {Albumentations: {Fast} and {Flexible} {Image} {Augmentations}},
	volume = {11},
	issn = {2078-2489},
	shorttitle = {Albumentations},
	url = {https://www.mdpi.com/2078-2489/11/2/125},
	doi = {10.3390/info11020125},
	abstract = {Data augmentation is a commonly used technique for increasing both the size and the diversity of labeled training sets by leveraging input transformations that preserve corresponding output labels. In computer vision, image augmentations have become a common implicit regularization technique to combat overfitting in deep learning models and are ubiquitously used to improve performance. While most deep learning frameworks implement basic image transformations, the list is typically limited to some variations of flipping, rotating, scaling, and cropping. Moreover, image processing speed varies in existing image augmentation libraries. We present Albumentations, a fast and flexible open source library for image augmentation with many various image transform operations available that is also an easy-to-use wrapper around other augmentation libraries. We discuss the design principles that drove the implementation of Albumentations and give an overview of the key features and distinct capabilities. Finally, we provide examples of image augmentations for different computer vision tasks and demonstrate that Albumentations is faster than other commonly used image augmentation tools on most image transform operations.},
	language = {en},
	number = {2},
	urldate = {2023-02-28},
	journal = {Information},
	author = {Buslaev, Alexander and Iglovikov, Vladimir I. and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A.},
	month = feb,
	year = {2020},
	pages = {125},
	file = {Submitted Version:C\:\\Users\\lblecher\\Zotero\\storage\\HNVVQIEX\\Buslaev et al. - 2020 - Albumentations Fast and Flexible Image Augmentati.pdf:application/pdf},
}

@book{march_calculus_1917,
	title = {Calculus},
	url = {http://archive.org/details/calculus00marciala},
	abstract = {xvi, 360 p. 19 cm},
	language = {eng},
	urldate = {2023-03-06},
	publisher = {New York : McGraw-Hill},
	author = {March, Herman W. (Herman William) and Wolff, Henry C. (Henry Charles)},
	collaborator = {{University of California Libraries}},
	year = {1917},
	keywords = {Calculus},
}

@article{maclean_new_2013,
	title = {A new approach for recognizing handwritten mathematics using relational grammars and fuzzy sets},
	volume = {16},
	issn = {1433-2825},
	url = {https://doi.org/10.1007/s10032-012-0184-x},
	doi = {10.1007/s10032-012-0184-x},
	abstract = {We present a new approach for parsing two-dimensional input using relational grammars and fuzzy sets. A fast, incremental parsing algorithm is developed, motivated by the two-dimensional structure of written mathematics. The approach reports all identifiable parses of the input. The parses are represented as a fuzzy set, in which the membership grade of a parse measures the similarity between it and the handwritten input. To identify and report parses efficiently, we adapt and apply existing techniques such as rectangular partitions and shared parse forests, and introduce new ideas such as relational classes and interchangeability. We also present a correction mechanism that allows users to navigate parse results and choose the correct interpretation in case of recognition errors or ambiguity. Such corrections are incorporated into subsequent incremental recognition results. Finally, we include two empirical evaluations of our recognizer. One uses a novel user-oriented correction count metric, while the other replicates the CROHME 2011 math recognition contest. Both evaluations demonstrate the effectiveness of our proposed approach.},
	language = {en},
	number = {2},
	urldate = {2023-03-09},
	journal = {International Journal on Document Analysis and Recognition (IJDAR)},
	author = {MacLean, Scott and Labahn, George},
	month = jun,
	year = {2013},
	keywords = {Membership Grade, Parse Table, Parse Tree, Relational Classis, Terminal Symbol},
	pages = {139--163},
	file = {Submitted Version:C\:\\Users\\lblecher\\Zotero\\storage\\IESZM3V4\\MacLean and Labahn - 2013 - A new approach for recognizing handwritten mathema.pdf:application/pdf},
}
@inproceedings{mahdavi_icdar_2019,
	address = {Sydney, Australia},
	title = {{ICDAR} 2019 {CROHME} + {TFD}: {Competition} on {Recognition} of {Handwritten} {Mathematical} {Expressions} and {Typeset} {Formula} {Detection}},
	isbn = {978-1-72813-014-9},
	shorttitle = {{ICDAR} 2019 {CROHME} + {TFD}},
	url = {https://ieeexplore.ieee.org/document/8978036/},
	doi = {10.1109/ICDAR.2019.00247},
	abstract = {We summarize the tasks, protocol, and outcome for the 6th Competition on Recognition of Handwritten Mathematical Expressions (CROHME), which includes a new formula detection in document images task (+ TFD). For CROHME + TFD 2019, participants chose between two tasks for recognizing handwritten formulas from 1) online stroke data, or 2) images generated from the handwritten strokes. To compare LATEX strings and the labeled directed trees over strokes (label graphs) used in previous CROHMEs, we convert LATEX and stroke-based label graphs to label graphs deﬁned over symbols (symbol-level label graphs, or symLG). More than thirty (33) participants registered for the competition, with nineteen (19) teams submitting results. The strongest formula recognition results were produced by the USTC-iFLYTEK research team, for both stroke-based (81\%) and image-based (77\%) input. For the new typeset formula detection task, the Samsung R\&D Institute Ukraine (Team 2) obtained a very strong F-score (93\%). System performance has improved since the last CROHME - still, the competition results suggest that recognition of handwritten formulae remains a difﬁcult structural pattern recognition task.},
	language = {en},
	urldate = {2023-03-09},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	publisher = {IEEE},
	author = {Mahdavi, Mahshad and Zanibbi, Richard and Mouchere, Harold and Viard-Gaudin, Christian and Garain, Utpal},
	month = sep,
	year = {2019},
	pages = {1533--1538},
	file = {Mahdavi et al. - 2019 - ICDAR 2019 CROHME + TFD Competition on Recognitio.pdf:C\:\\Users\\lblecher\\Zotero\\storage\\TBBAXWDS\\Mahdavi et al. - 2019 - ICDAR 2019 CROHME + TFD Competition on Recognitio.pdf:application/pdf},
}

@misc{sorscher_beyond_2022,
	title = {Beyond neural scaling laws: beating power law scaling via data pruning},
	shorttitle = {Beyond neural scaling laws},
	url = {http://arxiv.org/abs/2206.14486},
	abstract = {Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how in theory we can break beyond power law scaling and potentially even reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this improved scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling in practice on ResNets trained on CIFAR-10, SVHN, and ImageNet. Next, given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.},
	urldate = {2023-03-09},
	publisher = {arXiv},
	author = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S.},
	month = nov,
	year = {2022},
	note = {arXiv:2206.14486 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\6IHBKEHY\\2206.html:text/html},
}

@inproceedings{fan_hierarchical_2018,
	address = {Melbourne, Australia},
	title = {Hierarchical {Neural} {Story} {Generation}},
	url = {https://aclanthology.org/P18-1082},
	doi = {10.18653/v1/P18-1082},
	abstract = {We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.},
	urldate = {2023-03-10},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Fan, Angela and Lewis, Mike and Dauphin, Yann},
	month = jul,
	year = {2018},
	pages = {889--898},
	file = {Full Text PDF:C\:\\Users\\lblecher\\Zotero\\storage\\EXLXBMRW\\Fan et al. - 2018 - Hierarchical Neural Story Generation.pdf:application/pdf},
}

@misc{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
	urldate = {2023-03-23},
	publisher = {arXiv},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = jan,
	year = {2019},
	note = {arXiv:1711.05101 [cs, math]
version: 3},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:C\:\\Users\\lblecher\\Zotero\\storage\\GA5VQ9Y5\\Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\UWHEYS8L\\1711.html:text/html},
}

@misc{shah_cycle-consistency_2019,
	title = {Cycle-{Consistency} for {Robust} {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/1902.05660},
	abstract = {Despite significant progress in Visual Question Answering over the years, robustness of today's VQA models leave much to be desired. We introduce a new evaluation protocol and associated dataset (VQA-Rephrasings) and show that state-of-the-art VQA models are notoriously brittle to linguistic variations in questions. VQA-Rephrasings contains 3 human-provided rephrasings for 40k questions spanning 40k images from the VQA v2.0 validation dataset. As a step towards improving robustness of VQA models, we propose a model-agnostic framework that exploits cycle consistency. Specifically, we train a model to not only answer a question, but also generate a question conditioned on the answer, such that the answer predicted for the generated question is the same as the ground truth answer to the original question. Without the use of additional annotations, we show that our approach is significantly more robust to linguistic variations than state-of-the-art VQA models, when evaluated on the VQA-Rephrasings dataset. In addition, our approach outperforms state-of-the-art approaches on the standard VQA and Visual Question Generation tasks on the challenging VQA v2.0 dataset.},
	urldate = {2023-03-23},
	publisher = {arXiv},
	author = {Shah, Meet and Chen, Xinlei and Rohrbach, Marcus and Parikh, Devi},
	month = feb,
	year = {2019},
	note = {arXiv:1902.05660 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\NEIQISM3\\1902.html:text/html},
}

@misc{noauthor_kinetics_1970,
	title = {Kinetics and {Thermodynamics} in {High}-{Temperature} {Gases}},
	url = {https://ntrs.nasa.gov/citations/19700022795},
	abstract = {Conference on kinetics and thermodynamics of combustion and high temperature gases},
	urldate = {2023-05-02},
	month = jan,
	year = {1970},
	note = {NTRS Report/Patent Number: N70-32106-116
NTRS Document ID: 19700022795
NTRS Research Center: Glenn Research Center (GRC)},
	keywords = {Thermodynamics And Combustion},
	file = {Snapshot:C\:\\Users\\lblecher\\Zotero\\storage\\RQTWUY3G\\19700022795.html:text/html},
}
